{"version":"NotebookV1","origId":503877321547433,"name":"Power-Plant-ML-Pipeline","language":"scala","commands":[{"version":"CommandV1","origId":503877321547435,"guid":"431419f5-43f7-4885-9d92-67d4c4fe9021","subtype":"command","commandType":"auto","position":1.0,"command":"%md #Power Plant ML Pipeline Application\nThis is an end-to-end example of using a number of different machine learning algorithms to solve a supervised regression problem.\n\n###Table of Contents\n\n- *Step 1: Business Understanding*\n- *Step 2: Load Your Data*\n- *Step 3: Explore Your Data*\n- *Step 4: Visualize Your Data*\n- *Step 5: Data Preparation*\n- *Step 6: Data Modeling*\n- *Step 7: Tuning and Evaluation*\n- *Step 8: Deployment*\n\n\n\n*We are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant.  Power generation is a complex process, and understanding and predicting power output is an important element in managing a plant and its connection to the power grid.*\n\nMore information about Peaker or Peaking Power Plants can be found on Wikipedia https://en.wikipedia.org/wiki/Peaking_power_plant\n\n\nGiven this business problem, we need to translate it to a Machine Learning task.  The ML task is regression since the label (or target) we are trying to predict is numeric.\n\n\nThe example data is provided by UCI at [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)\n\nYou can read the background on the UCI page, but in summary we have collected a number of readings from sensors at a Gas Fired Power Plant\n\n(also called a Peaker Plant) and now we want to use those sensor readings to predict how much power the plant will generate.\n\n\nMore information about Machine Learning with Spark can be found in the programming guide in the [SparkML Guide](https://spark.apache.org/docs/latest/mllib-guide.html)\n\n\n*Please note this example only works with Spark version 1.4 or higher*","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1e2c643e-5c0e-4141-a21b-5edd9cd7c51c"},{"version":"CommandV1","origId":503877321547436,"guid":"5571e0fc-da3e-4829-b440-11cea67e31be","subtype":"command","commandType":"auto","position":2.0,"command":"require(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"17747c6d-c5fe-4479-ad05-c21d4bc00e31"},{"version":"CommandV1","origId":503877321547437,"guid":"2420f274-71b8-4e6d-9ed8-04f77008ff1a","subtype":"command","commandType":"auto","position":3.0,"command":"%md ##Step 1: Business Understanding\nThe first step in any machine learning task is to understand the business need. \n\nAs described in the overview we are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant.\n\nThe problem is a regression problem since the label (or target) we are trying to predict is numeric","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"477b76b9-aed6-4180-8f28-c020b2b63c5f"},{"version":"CommandV1","origId":503877321547438,"guid":"2f324672-51a2-4edf-9a04-776c0ef57506","subtype":"command","commandType":"auto","position":4.0,"command":"%md ##Step 2: Load Your Data\nNow that we understand what we are trying to do, we need to load our data and describe it, explore it and verify it.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a411328e-9b87-4610-b1e6-1f2306e5ba84"},{"version":"CommandV1","origId":503877321547439,"guid":"5e95bd04-66ba-451c-a95e-597fcfe062e5","subtype":"command","commandType":"auto","position":5.0,"command":"%md\nSince the dataset is relatively small, we will use the upload feature in Databricks to upload the data as a table.\n\nFirst download the Data Folder from [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)\n\nThe file is a multi-tab Excel document so you will need to save each tab as a Text file export. \n\nI prefer exporting as a Tab-Separated-Values (TSV) since it is more consistent than CSV.\n\nCall each file Folds5x2_pp<Sheet 1..5>.tsv and save to your machine.\n\nGo to the Databricks Menu > Tables > Create Table\n\nSelect Datasource as \"File\"\n\nUpload *ALL* 5 files at once.\n\nSee screenshots below:\n\n\n**2.1. Create Table**\n  _________________\n\nWhen you import your data, name your table `power_plant`, specify all of the columns with the datatype `Double` and make sure you check the `First row is header` box.\n\n![alt text](http://training.databricks.com/databricks_guide/1_4_ML_Power_Plant_Import_Table.png)\n\n**2.2. Review Schema**\n  __________________\n\nYour table schema and preview should look like this after you click ```Create Table```:\n\n![alt text](http://training.databricks.com/databricks_guide/1_4_ML_Power_Plant_Import_Table_Schema.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4bd33040-0cab-46df-8221-7dd42ab4b368"},{"version":"CommandV1","origId":503877321547440,"guid":"90a01760-c78f-481c-86c7-bb09d18f7912","subtype":"command","commandType":"auto","position":6.0,"command":"%md Now that your data is loaded let's explore it.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fdd6f329-55de-4b5e-9c76-00cbc9a5e943"},{"version":"CommandV1","origId":503877321547441,"guid":"7041badb-b470-4b54-a544-69d363ec6cdf","subtype":"command","commandType":"auto","position":7.0,"command":"%md #### Step 2: (Load your Data Alternative Option):\n\nIf you did Step 2 already you can skip down to Step 3. If you want to skip the data import and just load the data from our public datasets in S3 use the cell below.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3ba18c7c-5f58-4d7c-b37d-a21884d4065a"},{"version":"CommandV1","origId":503877321547442,"guid":"4a50889b-9e98-449d-abd8-9318fe77c6b8","subtype":"command","commandType":"auto","position":8.0,"command":"sqlContext.sql(\"DROP TABLE IF EXISTS power_plant\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/power_plant\", true)\ncase class PowerPlantTable(AT: Double, V : Double, AP : Double, RH : Double, PE : Double)\nval rawTextRdd = sc.textFile(\"dbfs:/databricks-datasets/power-plant/data/\")\nrawTextRdd\n  .map(x => x.split(\"\\t\"))\n  .filter(line => line(0) != \"AT\")\n  .map(line => PowerPlantTable(line(0).toDouble, line(1).toDouble, line(2).toDouble, line(3).toDouble, line(4).toDouble))\n  .toDF()\n  .write.saveAsTable(\"power_plant\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1b9c3e21-ef07-4bb1-88f6-5f97d8363827"},{"version":"CommandV1","origId":503877321547443,"guid":"aa5bc2bd-f2ed-4bab-9aef-70aceb308ca5","subtype":"command","commandType":"auto","position":9.0,"command":"%md ##Step 3: Explore Your Data\nNow that we understand what we are trying to do, we need to load our data and describe it, explore it and verify it.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8fdf2a6e-47f0-4e36-891c-fe298ddc75fc"},{"version":"CommandV1","origId":503877321547444,"guid":"3dbc3178-de2d-48f3-8302-3438dedfae6b","subtype":"command","commandType":"auto","position":10.0,"command":"%sql \n--We can use %sql to query the rows\n\nSELECT * FROM power_plant","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ed2f8c08-155e-43de-9707-80b502fb3cac"},{"version":"CommandV1","origId":503877321547445,"guid":"e056f251-2809-452d-b98b-b715a454a65f","subtype":"command","commandType":"auto","position":11.0,"command":"%md We can use the SQL desc command to describe the schema","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"18d78aea-33d8-439f-bbb3-00b2e53a4637"},{"version":"CommandV1","origId":503877321547446,"guid":"398d4a72-d63a-4ee0-9905-01ce68ea80cb","subtype":"command","commandType":"auto","position":12.0,"command":"%sql desc power_plant","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"023ea801-23e9-4d37-ab94-24326d045d4e"},{"version":"CommandV1","origId":503877321547447,"guid":"4d064245-3ae1-449c-911c-e73e0ae24943","subtype":"command","commandType":"auto","position":13.0,"command":"%md **Schema Definition**\n\nOur schema definition from UCI appears below:\n\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vaccum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output\n\nPE is our label or target. This is the value we are trying to predict given the measurements.\n\n*Reference [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5b225d00-b9ca-4996-a7d8-69d1b426a805"},{"version":"CommandV1","origId":503877321547448,"guid":"7ed2f3c9-1211-4b38-a493-43b7fc486ad9","subtype":"command","commandType":"auto","position":14.0,"command":"%md Let's do some basic statistical analysis of all the columns. \n\nWe can use the describe function with no parameters to get some basic stats for each column like count, mean, max, min and standard deviation. The describe function is a method attached to a dataframe. More information can be found in the [Spark API docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e0b5eddc-7296-4540-bd06-85532164ead7"},{"version":"CommandV1","origId":503877321547449,"guid":"0c06472b-6aba-467d-b705-ca377fd6ef12","subtype":"command","commandType":"auto","position":15.0,"command":"display(sqlContext.table(\"power_plant\").describe())","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9f02bfb2-6d0d-454d-bbd9-508a846a30f6"},{"version":"CommandV1","origId":503877321547450,"guid":"086b2047-a162-4a23-8e4d-854d06e05774","subtype":"command","commandType":"auto","position":16.0,"command":"%md ##Step 4: Visualize Your Data\n\nTo understand our data, we will look for correlations between features and the label.  This can be important when choosing a model.  E.g., if features and a label are linearly correlated, a linear model like Linear Regression can do well; if the relationship is very non-linear, more complex models such as Decision Trees can be better. We use Databrick's built in visualization to view each of our predictors in relation to the label column as a scatter plot to see the correlation between the predictors and the label.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6854dd9d-2e02-41d7-8a36-a97a2baede8c"},{"version":"CommandV1","origId":503877321547451,"guid":"b2b009ad-72e6-48a7-aa6d-34e6d7477844","subtype":"command","commandType":"auto","position":17.0,"command":"%sql select AT as Temperature, PE as Power from power_plant","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cbf183b7-e7b0-43e3-8189-f2f0bb5a069f"},{"version":"CommandV1","origId":503877321547452,"guid":"1e3394c6-a25b-415f-b2d4-099f9366190d","subtype":"command","commandType":"auto","position":18.0,"command":"%md It looks like there is strong linear correlation between temperature and Power Output","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3ed15d49-9e78-4aef-9ced-58e515ef5fb5"},{"version":"CommandV1","origId":503877321547453,"guid":"e931774f-af95-4f9b-a301-a1cf95138ecb","subtype":"command","commandType":"auto","position":19.0,"command":"%sql select V as ExhaustVaccum, PE as Power from power_plant;","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4cf33561-fcd6-4915-b86f-94f7e451b7e6"},{"version":"CommandV1","origId":503877321547454,"guid":"c2c31a76-677d-4c3e-af8c-1041f756e7a7","subtype":"command","commandType":"auto","position":20.0,"command":"%md The linear correlation is not as strong between Exhaust Vacuum Speed and Power Output but there is some semblance of a pattern.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5f52dee9-78aa-4af1-92b8-773c2be010a3"},{"version":"CommandV1","origId":503877321547455,"guid":"25a98a14-8a69-4a91-891e-74e7ef7c1ad6","subtype":"command","commandType":"auto","position":21.0,"command":"%sql select AP Pressure, PE as Power from power_plant;","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b2551c13-a562-4f04-9b88-28fe26562514"},{"version":"CommandV1","origId":503877321547456,"guid":"cbf76cea-d724-4de2-92b9-13a802601df2","subtype":"command","commandType":"auto","position":22.0,"command":"%sql select RH Humidity, PE Power from power_plant;","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"17fe0c3b-e9b7-4b83-99b3-74e0b9dd9fdb"},{"version":"CommandV1","origId":503877321547457,"guid":"76e5d7f3-7064-4411-a1ca-049a649b5e64","subtype":"command","commandType":"auto","position":23.0,"command":"%md ...and atmospheric pressure and relative humidity seem to have little to no linear correlation","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"19b961b3-6762-4c67-b0fc-dd81364ad686"},{"version":"CommandV1","origId":503877321547458,"guid":"12f49051-cad8-46f6-a5df-6ebd8c6f2655","subtype":"command","commandType":"auto","position":24.0,"command":"%md ##Step 5: Data Preparation\n\nThe next step is to prepare the data. Since all of this data is numeric and consistent this is a simple task for us today.\n\nWe will need to convert the predictor features from columns to Feature Vectors using the org.apache.spark.ml.feature.VectorAssembler\n\nThe VectorAssembler will be the first step in building our ML pipeline.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"85d7dd5a-4a5a-44e8-8d36-59993d169683"},{"version":"CommandV1","origId":503877321547459,"guid":"c180b5f6-6a88-471c-a781-a4f3a3f62105","subtype":"command","commandType":"auto","position":25.0,"command":"import org.apache.spark.ml.feature.VectorAssembler\n\nval dataset = sqlContext.table(\"power_plant\")\n\nval vectorizer = new VectorAssembler()\nvectorizer.setInputCols(Array(\"AT\", \"V\", \"AP\", \"RH\"))\nvectorizer.setOutputCol(\"features\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"32ae8874-3dbd-4c03-90e5-36c025e8979b"},{"version":"CommandV1","origId":503877321547460,"guid":"e13aac5f-493c-4cb8-a2b7-42185878f8c5","subtype":"command","commandType":"auto","position":26.0,"command":"%md ##Step 6: Data Modeling\nNow let's model our data to predict what the power output will be given a set of sensor readings\n\nOur first model will be based on simple linear regression since we saw some linear patterns in our data based on the scatter plots during the exploration stage.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6b866939-8185-4d7a-ad72-79774aa69afe"},{"version":"CommandV1","origId":503877321547461,"guid":"2848d08e-6348-4668-b84f-3c47d9f907ef","subtype":"command","commandType":"auto","position":27.0,"command":"// First let's hold out 20% of our data for testing and leave 80% for training\n\nvar Array(split20, split80) = dataset.randomSplit(Array(0.20, 0.80), 1800009193L)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"31e85389-67f2-4aac-bf6b-4409464e3ac9"},{"version":"CommandV1","origId":503877321547462,"guid":"d4e63034-4c7a-46db-9213-ca2278bd2842","subtype":"command","commandType":"auto","position":28.0,"command":"// Let's cache these datasets for performance\nval testSet = split20.cache()\nval trainingSet = split80.cache()","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1bb5d489-8d62-4205-98f7-0cf105c980d7"},{"version":"CommandV1","origId":503877321547463,"guid":"83b76069-4a4c-40ab-baf4-f97ec191b661","subtype":"command","commandType":"auto","position":29.0,"command":"// ***** LINEAR REGRESSION MODEL ****\n\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.regression.LinearRegressionModel\nimport org.apache.spark.ml.Pipeline\n\n// Let's initialize our linear regression learner\nval lr = new LinearRegression()","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"215c3c0e-5354-4ab8-87de-38e636a0538c"},{"version":"CommandV1","origId":503877321547464,"guid":"338d1714-b13b-4651-8fc4-acf845a46789","subtype":"command","commandType":"auto","position":30.0,"command":"// We use explain params to dump the parameters we can use\nlr.explainParams()","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4152f7da-67d9-40ef-bff3-040fad78dd49"},{"version":"CommandV1","origId":503877321547465,"guid":"c141b6c5-9ee1-4e5e-a0c7-c5403bbbd2f6","subtype":"command","commandType":"auto","position":31.0,"command":"%md The cell below is based on the Spark ML pipeline API. More information can be found in the Spark ML Programming Guide at https://spark.apache.org/docs/latest/ml-guide.html","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9d6f59fc-8fbb-4c7b-b715-c75b8b77817b"},{"version":"CommandV1","origId":503877321547466,"guid":"d5494e79-b70a-474e-a7af-0c969cf7704e","subtype":"command","commandType":"auto","position":32.0,"command":"// Now we set the parameters for the method\nlr.setPredictionCol(\"Predicted_PE\")\n  .setLabelCol(\"PE\")\n  .setMaxIter(100)\n  .setRegParam(0.1)\n\n\n// We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\nval lrPipeline = new Pipeline()\n\nlrPipeline.setStages(Array(vectorizer, lr))\n\n\n// Let's first train on the entire dataset to see what we get\nval lrModel = lrPipeline.fit(trainingSet)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d9f8288f-1371-4837-bd85-5767ae333361"},{"version":"CommandV1","origId":503877321547467,"guid":"fbdd6c9b-e97e-41f5-a6a3-2db96a3414db","subtype":"command","commandType":"auto","position":33.0,"command":"%md \nSince Linear Regression is Simply a Line of best fit over the data that minimizes the square of the error, given multiple input dimensions we can express each predictor as a line function of the form:\n\n%[ y = a + b x_1 + b x_2 + b x_i ... ]%\n\nwhere a is the intercept and b are coefficients.\n\nTo express the coefficients of that line we can retrieve the Estimator stage from the PipelineModel and express the weights and the intercept for the function.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"48c8ed26-dd81-48cf-8de0-d28171c2c542"},{"version":"CommandV1","origId":503877321547468,"guid":"3c62f232-d230-4d8b-b214-7d19f1e8b323","subtype":"command","commandType":"auto","position":34.0,"command":"// The intercept is as follows:\nval intercept = lrModel.stages(1).asInstanceOf[LinearRegressionModel].intercept","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"10674310-aaa7-4b0d-8c6b-56c36eed854b"},{"version":"CommandV1","origId":503877321547469,"guid":"9819b0c4-9eca-40ea-8539-9c06f08f8c23","subtype":"command","commandType":"auto","position":35.0,"command":"// The coefficents (i.e. weights) are as follows:\n\nval weights = lrModel.stages(1).asInstanceOf[LinearRegressionModel].weights.toArray\n\n\nval featuresNoLabel = dataset.columns.filter(col => col != \"PE\")\n\nval coefficents = sc.parallelize(weights).zip(sc.parallelize(featuresNoLabel))\n\n// Now let's sort the coeffecients from the most to the least\n\nvar equation = s\"y = $intercept \"\nvar variables = Array\ncoefficents.sortByKey().collect().foreach(x =>\n  { \n        val weight = Math.abs(x._1)\n        val name = x._2\n        val symbol = if (x._1 > 0) \"+\" else \"-\"\n          \n        equation += (s\" $symbol (${weight} * ${name})\")\n  } \n)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0804de06-800a-484e-862b-ae25b5baeb9d"},{"version":"CommandV1","origId":503877321547470,"guid":"5ecf9d2b-5b67-4dff-907e-ffb3f5ddc64e","subtype":"command","commandType":"auto","position":36.0,"command":"// Finally here is our equation\nprintln(\"Linear Regression Equation: \" + equation)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"615b1ab0-fde9-4218-bf47-d2f63c242252"},{"version":"CommandV1","origId":503877321547471,"guid":"fac239b2-ce67-4db4-888f-3becbf8c3171","subtype":"command","commandType":"auto","position":37.0,"command":"%md Based on examining the output it shows there is a strong negative correlation between Atmospheric Temperature (AT) and Power Output.\n\nBut our other dimenensions seem to have little to no correlation with Power Output. Do you remember **Step 2: Explore Your Data**? When we visualized each predictor against Power Output using a Scatter Plot, only the temperature variable seemed to have a linear correlation with Power Output so our final equation seems logical.\n\n\nNow let's see what our predictions look like given this model.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cfb1d87c-582d-4743-b736-e5ac4bfbd710"},{"version":"CommandV1","origId":503877321547472,"guid":"8971ca02-1abc-42ae-ae08-fb2a7329333f","subtype":"command","commandType":"auto","position":38.0,"command":"val predictionsAndLabels = lrModel.transform(testSet)\n\ndisplay(predictionsAndLabels.select(\"AT\", \"V\", \"AP\", \"RH\", \"PE\", \"Predicted_PE\"))","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2e4b1513-ce5b-47f7-abca-918e28fd68a6"},{"version":"CommandV1","origId":503877321547473,"guid":"ab2912a3-7187-490d-a7b9-84ee0017c67e","subtype":"command","commandType":"auto","position":39.0,"command":"%md Now that we have real predictions we can use an evaluation metric such as Root Mean Squared Error to validate our regression model. The lower the Root Mean Squared Error, the better our model.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"32b6018d-bd3f-443e-b838-e33a685067d1"},{"version":"CommandV1","origId":503877321547474,"guid":"c829cf12-9104-4b5d-9b25-436b50f735ca","subtype":"command","commandType":"auto","position":40.0,"command":"//Now let's compute some evaluation metrics against our test dataset\n\nimport org.apache.spark.mllib.evaluation.RegressionMetrics \n\nval metrics = new RegressionMetrics(predictionsAndLabels.select(\"Predicted_PE\", \"PE\").rdd.map(r => (r(0).asInstanceOf[Double], r(1).asInstanceOf[Double])))\n\nval rmse = metrics.rootMeanSquaredError\nval explainedVariance = metrics.explainedVariance\nval r2 = metrics.r2\n\nprintln (f\"Root Mean Squared Error: $rmse\")\nprintln (f\"Explained Variance: $explainedVariance\")  \nprintln (f\"R2: $r2\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"948b31bd-105d-470e-846c-3a20ef10565c"},{"version":"CommandV1","origId":503877321547475,"guid":"136d429a-12f8-412c-991b-d60a3396e8b1","subtype":"command","commandType":"auto","position":41.0,"command":"%md Generally a good model will have 68% of predictions within 1 RMSE and 95% within 2 RMSE of the actual value. Let's calculate and see if a RMSE of 4.51 meets this criteria.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c8ae56e5-2509-4972-a672-50a95c359f37"},{"version":"CommandV1","origId":503877321547476,"guid":"18441366-7763-4b87-a1f9-30c89ca750b6","subtype":"command","commandType":"auto","position":42.0,"command":"// First we calculate the residual error and divide it by the RMSE\npredictionsAndLabels.selectExpr(\"PE\", \"Predicted_PE\", \"PE - Predicted_PE Residual_Error\", s\"\"\" abs(PE - Predicted_PE) / $rmse Within_RSME\"\"\").registerTempTable(\"Power_Plant_RMSE_Evaluation\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a3897667-85cd-4fcc-87ce-173b3cdbb369"},{"version":"CommandV1","origId":503877321547477,"guid":"35d09eb6-26e1-4c9e-bc1a-acdef3104324","subtype":"command","commandType":"auto","position":43.0,"command":"%sql SELECT * from Power_Plant_RMSE_Evaluation","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bd309900-68ba-42be-af10-0496161b6251"},{"version":"CommandV1","origId":503877321547478,"guid":"ef75ba37-a137-4d10-894c-02be117c5559","subtype":"command","commandType":"auto","position":44.0,"command":"%sql -- Now we calculate % of rows within 1 RMSE, within 2 RMSEs and greater than 2 RMSEs\n-- We use a pie chart to visualize the results\nSELECT ceiling(Within_RSME) Within_RSME, count(*) count  from Power_Plant_RMSE_Evaluation GROUP BY ceiling(Within_RSME)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e4eaca3e-5a2e-4170-848d-fd93d0a29a92"},{"version":"CommandV1","origId":503877321547479,"guid":"f9eb919f-b150-4db1-bd68-b0beb6d8f749","subtype":"command","commandType":"auto","position":45.0,"command":"%md So we have 68% of our training data within 1 RMSE and 97% (68% + 29%) within 2 RMSE. So the model is pretty decent. Let's see if we can tune the model to improve it further.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bd6b3ca1-d726-425e-8143-97e4d7aff435"},{"version":"CommandV1","origId":503877321547480,"guid":"5f918b6b-61da-46bc-b1e5-2b4607285bf0","subtype":"command","commandType":"auto","position":46.0,"command":"%md #Step 7: Tuning and Evaluation\n\nNow that we have a model with all of the data let's try to make a better model by tuning over several parameters to see if we can get better results.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"81d52f4a-a25a-41c6-9173-71a949aa007f"},{"version":"CommandV1","origId":503877321547481,"guid":"ef2ff887-af80-43fd-b931-5cdc4424bc64","subtype":"command","commandType":"auto","position":47.0,"command":"import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\nimport org.apache.spark.ml.evaluation._\n//first let's use a cross validator to split the data into training and validation subsets\n\n\n//Let's set up our evaluator class to judge the model based on the best root mean squared error\nval regEval = new RegressionEvaluator()\nregEval.setLabelCol(\"PE\")\n  .setPredictionCol(\"Predicted_PE\")\n  .setMetricName(\"rmse\")\n\n//Let's create our crossvalidator with 3 fold cross validation\nval crossval = new CrossValidator()\ncrossval.setEstimator(lrPipeline)\ncrossval.setNumFolds(5)\ncrossval.setEvaluator(regEval)\n\n\n//Let's tune over our regularization parameter from 0.01 to 0.10\nval regParam = ((1 to 10) toArray).map(x => (x /100.0))\n\nval paramGrid = new ParamGridBuilder()\n  .addGrid(lr.regParam, regParam)\n  .build()\ncrossval.setEstimatorParamMaps(paramGrid)\n\n//Now let's create our model\nval cvModel = crossval.fit(trainingSet)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c663962f-bad3-42fb-9dec-9d7bc3a5c8a3"},{"version":"CommandV1","origId":503877321547482,"guid":"75927a4d-c8b6-4bb6-b8a4-d053ebd0630f","subtype":"command","commandType":"auto","position":48.0,"command":"%md Now that we have tuned let's see what we got for tuning parameters and what our RMSE was versus our intial model","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f5355981-d516-4915-bba8-59ba8d20feba"},{"version":"CommandV1","origId":503877321547483,"guid":"f37ac7a0-74a4-431e-aa85-e355edd3ed80","subtype":"command","commandType":"auto","position":49.0,"command":"val predictionsAndLabels = cvModel.transform(testSet)\nval metrics = new RegressionMetrics(predictionsAndLabels.select(\"Predicted_PE\", \"PE\").rdd.map(r => (r(0).asInstanceOf[Double], r(1).asInstanceOf[Double])))\n\nval rmse = metrics.rootMeanSquaredError\nval explainedVariance = metrics.explainedVariance\nval r2 = metrics.r2\n\nprintln (f\"Root Mean Squared Error: $rmse\")\nprintln (f\"Explained Variance: $explainedVariance\")  \nprintln (f\"R2: $r2\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"88652503-53ff-4cf0-ba19-ce42302002fc"},{"version":"CommandV1","origId":503877321547484,"guid":"76e245f9-a564-49ff-b0cf-163b7a36a5e6","subtype":"command","commandType":"auto","position":50.0,"command":"%md So our initial untuned and tuned linear regression models are statistically identical.\n\nGiven that the only linearly correlated variable is Temperature, it makes sense try another machine learning method such a Decision Tree to handle non-linear data and see if we can improve our model\n\nA Decision Tree creates a model based on splitting variables using a tree structure. We will first start with a single decision tree model.\n\nReference Decision Trees: https://en.wikipedia.org/wiki/Decision_tree_learning","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ca3e9d17-19d9-454f-bc70-37578dc02505"},{"version":"CommandV1","origId":503877321547485,"guid":"01cc6221-3ab8-41a0-9aca-3160132530c4","subtype":"command","commandType":"auto","position":51.0,"command":"import org.apache.spark.ml.regression.DecisionTreeRegressor\n\n\nval dt = new DecisionTreeRegressor()\ndt.setLabelCol(\"PE\")\ndt.setPredictionCol(\"Predicted_PE\")\ndt.setFeaturesCol(\"features\")\ndt.setMaxBins(100)\n\nval dtPipeline = new Pipeline()\ndtPipeline.setStages(Array(vectorizer, dt))\n//Let's just resuse our CrossValidator\n\ncrossval.setEstimator(dtPipeline)\n\nval paramGrid = new ParamGridBuilder()\n  .addGrid(dt.maxDepth, Array(2, 3))\n  .build()\ncrossval.setEstimatorParamMaps(paramGrid)\n\nval dtModel = crossval.fit(trainingSet)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fefd8ea0-ff6b-4d27-829c-79aeea0b6607"},{"version":"CommandV1","origId":503877321547486,"guid":"0e8d7ccd-1cd6-4da6-a609-98aba0afe5fe","subtype":"command","commandType":"auto","position":52.0,"command":"%md Now let's see how our DecisionTree model compares to our LinearRegression model","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a09bacac-1f39-46f6-8642-7faf9c92908a"},{"version":"CommandV1","origId":503877321547487,"guid":"2749eeb7-805f-4b1d-9d0d-aadd1e31ffed","subtype":"command","commandType":"auto","position":53.0,"command":"import org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.PipelineModel\n \n\nval predictionsAndLabels = dtModel.bestModel.transform(testSet)\nval metrics = new RegressionMetrics(predictionsAndLabels.select(\"Predicted_PE\", \"PE\").map(r => (r(0).asInstanceOf[Double], r(1).asInstanceOf[Double])))\n\nval rmse = metrics.rootMeanSquaredError\nval explainedVariance = metrics.explainedVariance\nval r2 = metrics.r2\n\nprintln (f\"Root Mean Squared Error: $rmse\")\nprintln (f\"Explained Variance: $explainedVariance\")  \nprintln (f\"R2: $r2\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"30d5b30c-d3c9-4baf-a142-f6527541f63f"},{"version":"CommandV1","origId":503877321547488,"guid":"0ad0f930-df57-4e92-8808-ecd55fd0040d","subtype":"command","commandType":"auto","position":54.0,"command":"// This line will pull the Decision Tree model from the Pipeline as display it as an if-then-else string\ndtModel.bestModel.asInstanceOf[PipelineModel].stages.last.asInstanceOf[DecisionTreeRegressionModel].toDebugString","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ac6dc3e7-63be-4f02-bbf8-0607d68bf655"},{"version":"CommandV1","origId":503877321547489,"guid":"e73f732b-ada8-452c-88f5-29791951a281","subtype":"command","commandType":"auto","position":55.0,"command":"%md So our DecisionTree was slightly worse than our LinearRegression model (LR: 4.51 vs DT: 5.03). Maybe we can try an Ensemble method such as Gradient-Boosted Decision Trees to see if we can strengthen our model by using an ensemble of weaker trees with weighting to reduce the error in our model.\n\n__Note__ since this is a complex model, the cell below can take about 3 minutes or so to run.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"342cbc17-9c00-4439-bb97-4c05187b76b0"},{"version":"CommandV1","origId":503877321547490,"guid":"0bfa6850-6686-45ec-8298-24e7071f75a2","subtype":"command","commandType":"auto","position":56.0,"command":"import org.apache.spark.ml.regression.GBTRegressor\n\nval gbt = new GBTRegressor()\ngbt.setLabelCol(\"PE\")\ngbt.setPredictionCol(\"Predicted_PE\")\ngbt.setFeaturesCol(\"features\")\ngbt.setSeed(100088121L)\ngbt.setMaxBins(30)\ngbt.setMaxIter(30)\n\nval gbtPipeline = new Pipeline()\ngbtPipeline.setStages(Array(vectorizer, gbt))\n//Let's just resuse our CrossValidator\n\ncrossval.setEstimator(gbtPipeline)\n\nval paramGrid = new ParamGridBuilder()\n  .addGrid(gbt.maxDepth, Array(2, 3))\n  .build()\ncrossval.setEstimatorParamMaps(paramGrid)\n\n//gbt.explainParams\nval gbtModel = crossval.fit(trainingSet)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"21ed2211-1683-4ecb-b1c5-ac3e64c8ff4a"},{"version":"CommandV1","origId":503877321547491,"guid":"2403b7fe-fe94-4fa6-a759-1ccd57e342c1","subtype":"command","commandType":"auto","position":57.0,"command":"import org.apache.spark.ml.regression.GBTRegressionModel \n\nval predictionsAndLabels = gbtModel.bestModel.transform(testSet)\nval metrics = new RegressionMetrics(predictionsAndLabels.select(\"Predicted_PE\", \"PE\").map(r => (r(0).asInstanceOf[Double], r(1).asInstanceOf[Double])))\n\nval rmse = metrics.rootMeanSquaredError\nval explainedVariance = metrics.explainedVariance\nval r2 = metrics.r2\n\n\nprintln (f\"Root Mean Squared Error: $rmse\")\nprintln (f\"Explained Variance: $explainedVariance\")  \nprintln (f\"R2: $r2\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"eafa3758-89cb-45f4-bd3f-222a187c0fd9"},{"version":"CommandV1","origId":503877321547492,"guid":"7035b5fd-0bbb-4236-bb30-aabe5b89cf8c","subtype":"command","commandType":"auto","position":58.0,"command":"%md We can use the toDebugString method to dump out what our trees and weighting look like:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4a3278d5-6788-43df-83b9-c93c00a2ca4e"},{"version":"CommandV1","origId":503877321547493,"guid":"26cc20b1-55b2-4cfe-aeba-1f96d0d8e46b","subtype":"command","commandType":"auto","position":59.0,"command":"gbtModel.bestModel.asInstanceOf[PipelineModel].stages.last.asInstanceOf[GBTRegressionModel].toDebugString","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8e11ae1b-fe44-4926-beb4-6d0f843bc5c6"},{"version":"CommandV1","origId":503877321547494,"guid":"58666d50-0675-4797-8667-de55a2f939f0","subtype":"command","commandType":"auto","position":60.0,"command":"%md ### Conclusion\n\nWow! So our best model is in fact our Gradient Boosted Decision tree model which uses an ensemble of 30 Trees with a depth of 3 to construct a better model than the single decision tree.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"176d8138-f387-498d-8abf-3e092c6d1a62"},{"version":"CommandV1","origId":503877321547495,"guid":"e97ac532-8c25-4d13-9d16-b87ace048cd8","subtype":"command","commandType":"auto","position":61.0,"command":"%md #Step 8: Deployment\n\nNow that we have a predictive model it is time to deploy the model into an operational environment. \n\nIn our example, let's say we have a series of sensors attached the power plant and a monitoring station.\n\nThe monitoring station will need close to real-time information about how much power that their station will generate so they can relay that to the utility. \n\nSo let's create a Spark Streaming utility that we can use for this purpose.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b07fc27d-d5f8-41e4-b8bc-fe5cafe24fc3"},{"version":"CommandV1","origId":503877321547496,"guid":"b7628732-925b-4a82-9c1a-cbffccaa3a96","subtype":"command","commandType":"auto","position":62.0,"command":"// Let's set the variable finalModel to our best GBT Model\nval finalModel = gbtModel.bestModel","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dc66b191-a6a2-4cbd-b03c-a2e83d0648ac"},{"version":"CommandV1","origId":503877321547497,"guid":"9f6867ce-8561-40ff-b10e-6d91da11f092","subtype":"command","commandType":"auto","position":63.0,"command":"%md Now let's create our streaming job to score new power plant readings in real-time","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ccb5552e-118b-4199-8147-f349b25e56ae"},{"version":"CommandV1","origId":503877321547498,"guid":"d4b13395-d6d9-418c-97d7-ba11f5f125d7","subtype":"command","commandType":"auto","position":64.0,"command":"import java.nio.ByteBuffer\nimport java.net._\nimport java.io._\nimport scala.io._\nimport sys.process._\nimport org.apache.spark.Logging\nimport org.apache.spark.SparkConf\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.Minutes\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport org.apache.spark.streaming.receiver.Receiver\nimport sqlContext._\nimport net.liftweb.json.DefaultFormats\nimport net.liftweb.json._\n\nimport scala.collection.mutable.SynchronizedQueue\n\n\nval queue = new SynchronizedQueue[RDD[String]]()\n\nval batchIntervalSeconds = 2\n\nvar newContextCreated = false      // Flag to detect whether new context was created or not\n\n// Function to create a new StreamingContext and set it up\ndef creatingFunc(): StreamingContext = {\n    \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n  val batchInterval = Seconds(1)\n  ssc.remember(Seconds(300))\n  val dstream = ssc.queueStream(queue)\n  dstream.foreachRDD { \n    rdd =>\n       if(!(rdd.isEmpty())) {\n           finalModel.transform(read.json(rdd).toDF()).write.mode(SaveMode.Append).saveAsTable(\"power_plant_predictions\")\n       } \n  }\n  println(\"Creating function called to create new StreamingContext for Power Plant Predictions\")\n  newContextCreated = true  \n  ssc\n}\n\nval ssc = StreamingContext.getActiveOrCreate(creatingFunc)\nif (newContextCreated) {\n  println(\"New context created from currently defined creating function\") \n} else {\n  println(\"Existing context running or recovered from checkpoint, may not be running currently defined creating function\")\n}\n\nssc.start()","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e5489345-13f0-483a-acf2-ef2e5218ba7a"},{"version":"CommandV1","origId":503877321547499,"guid":"2b63bc0a-84d9-45a7-b3f0-6e0e161e930c","subtype":"command","commandType":"auto","position":65.0,"command":"%md Now that we have created and defined our streaming job, let's test it with some data. First we clear the predictions table.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3d5e9e2c-e40a-41cd-9447-580e476a5eb8"},{"version":"CommandV1","origId":503877321547500,"guid":"356254e5-de48-4ef4-9546-b063469596d5","subtype":"command","commandType":"auto","position":66.0,"command":"%sql truncate table power_plant_predictions","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cc340b65-3ef3-4bb6-90af-7ca78c6651cb"},{"version":"CommandV1","origId":503877321547501,"guid":"f9a8f309-1500-4320-9584-2b98f3d7d984","subtype":"command","commandType":"auto","position":67.0,"command":"%md Let's use data to see how much power output our model will predict.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b23fa6ac-3ceb-4b50-a259-82a6371fd575"},{"version":"CommandV1","origId":503877321547502,"guid":"2ccd0d95-3987-4a69-813f-69080c7457f9","subtype":"command","commandType":"auto","position":68.0,"command":"// First we try it with a record from our test set and see what we get:\nqueue += sc.makeRDD(Seq(s\"\"\"{\"AT\":10.82,\"V\":37.5,\"AP\":1009.23,\"RH\":96.62,\"PE\":473.9}\"\"\"))","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fc985f0e-8204-405b-9897-2ce7b13b46e6"},{"version":"CommandV1","origId":503877321547503,"guid":"6494b086-d112-45af-90bb-b902f0ee2b23","subtype":"command","commandType":"auto","position":69.0,"command":"%sql \n--and we can query our predictions table\nselect * from power_plant_predictions","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"afaa8627-c456-40c3-b2dd-a63a13dbb097"},{"version":"CommandV1","origId":503877321547504,"guid":"d53d4214-ccf6-4872-a252-9b05af6da68c","subtype":"command","commandType":"auto","position":70.0,"command":"%md Let's repeat with a different test measurement that our model has not seen before","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6da578d2-e1c1-4ea9-815d-d798ed5eded6"},{"version":"CommandV1","origId":503877321547505,"guid":"54160ca8-1110-4f69-9d65-ace9b79242c8","subtype":"command","commandType":"auto","position":71.0,"command":"queue += sc.makeRDD(Seq(s\"\"\"{\"AT\":10.0,\"V\":40,\"AP\":1000,\"RH\":90.0,\"PE\":0.0}\"\"\"))","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b240e112-1110-4e89-af97-1d7333bdb8da"},{"version":"CommandV1","origId":503877321547506,"guid":"64ce7306-ac66-4668-ba29-7a7d0015cf67","subtype":"command","commandType":"auto","position":72.0,"command":"%sql \n--Note you made to run this a couple of times to see the refreshed data...\nselect * from power_plant_predictions","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"276c7144-3c8f-4170-a9b8-b0f9b5a3b850"},{"version":"CommandV1","origId":503877321547507,"guid":"dc29c999-f6ae-4133-a642-23265d364371","subtype":"command","commandType":"auto","position":73.0,"command":"%md As you can see the Predictions are very close to the real data points. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b3bba0a3-d0f3-4c93-a50d-ee89b5616f7e"},{"version":"CommandV1","origId":503877321547508,"guid":"2900b91e-0930-407f-9ebf-5d4a9ba40694","subtype":"command","commandType":"auto","position":74.0,"command":"%sql select * from power_plant where at between 10 and 11 and AP between 1000 and 1010 and RH between 90 and 97 and v between 37 and 40 order by PE ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d9495ff0-0f09-445c-ac61-789f1f7a830f"},{"version":"CommandV1","origId":503877321547509,"guid":"c5476482-ad4e-4854-97d4-19cab163e8bd","subtype":"command","commandType":"auto","position":75.0,"command":"ssc.stop(false)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"99e2e681-9721-402e-ba2d-618d0a4eecb5"},{"version":"CommandV1","origId":503877321547510,"guid":"3720dd55-b674-4493-8304-77a7772b15ff","subtype":"command","commandType":"auto","position":76.0,"command":"%md Now you use the predictions table to feed a real-time dashboard or feed the utility with information on how much power the peaker plant will deliver.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"790082a5-4953-457c-beea-44ebc8686bb5"},{"version":"CommandV1","origId":503877321547511,"guid":"465ab45a-3d44-421d-964f-4c2797e415f6","subtype":"command","commandType":"auto","position":77.0,"command":"%md Datasource Reference:\nPinar Tüfekci, Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods, International Journal of Electrical Power & Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615, [Web Link].\n([Web Link])\n\nHeysem Kaya, Pinar Tüfekci , Fikret Gürgen: Local and Global Learning Methods for Predicting Power of a Combined Gas & Steam Turbine, Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a3f85c09-2362-47fe-b183-40634df84c58"}],"dashboards":[],"guid":"87d10673-ecba-48d0-9fe6-4c762ee9a20f","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}}