{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia: TF-IDF with Normalization for K-Means\n",
    " \n",
    "In this lab, we explore generating a k-means model to cluster Wikipedia articles.  This clustering could be used as part of an exploratory data analysis (EDA) process or as a way to build features for a supervised learning technique.\n",
    " \n",
    "We'll create a `Pipeline` that can be used to make the cluster predictions.  This lab will make use of `RegexTokenizer`, `HashingTF`, `IDF`, `Normalizer`, `Pipeline`, and `KMeans`.  You'll also see how to perform a stratified random sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfSmall = sqlContext.read.load('/mnt/ml-class/smallwiki.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out non-relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "parsed = dfSmall.filter((col('title') != '<PARSE ERROR>') &\n",
    "                           col('redirect_title').isNull() &\n",
    "                           col('text').isNotNull())\n",
    "parsed.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a regular expression to tokenize (split into words).  Pattern defaults to matching the separator, but can be set to match tokens instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "tokenizer = (RegexTokenizer()\n",
    "             .setInputCol(\"text\")\n",
    "             .setOutputCol(\"words\")\n",
    "             .setPattern(\"\\\\W+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `HashingTF` transformer to hash words to buckets with counts, then use an `IDF` estimator to compute inverse-document frequency for buckets based on how frequently words have hashed to those buckets in the given documents.  Next, normalize the tf-idf values so that the \\\\( l^2 \\\\) norm is one for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF, HashingTF, Normalizer\n",
    "\n",
    "hashingTF = (HashingTF()\n",
    "             .setNumFeatures(10000)\n",
    "             .setInputCol(tokenizer.getOutputCol())\n",
    "             .setOutputCol('hashingTF'))\n",
    "\n",
    "idf = (IDF()\n",
    "       .setMinDocFreq(10)\n",
    "       .setInputCol(hashingTF.getOutputCol())\n",
    "       .setOutputCol('idf'))\n",
    "\n",
    "normalizer = (Normalizer()\n",
    "              .setInputCol(idf.getOutputCol())\n",
    "              .setOutputCol('features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the `KMeans` estimator and a `Pipeline` that will contain all of the stages.  We'll then call fit on the `Pipeline` which will give us back a `PipelineModel`.  This will take about a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = (KMeans()\n",
    "          .setFeaturesCol('features')\n",
    "          .setPredictionCol('prediction')\n",
    "          .setK(5)\n",
    "          .setSeed(0))\n",
    "\n",
    "pipeline = Pipeline().setStages([tokenizer, hashingTF, idf, normalizer, kmeans])\n",
    "model = pipeline.fit(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample of the data to see if we can see a pattern between predicted clusters and titles.  We'll use a stratified sample to over-weight the less frequent predictions for inspection purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(parsed)\n",
    "stratifiedMap = {0: .03, 1: .04, 2: .06, 3: .40, 4: .005}\n",
    "sampleDF = predictions.sampleBy('prediction', stratifiedMap, 0)\n",
    "display(sampleDF.select('title', 'prediction').orderBy('prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(predictions.select(\"features\"))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
