{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia: Word2Vec\n",
    " \n",
    "In this lab, we'll use `Word2Vec` to create vectors the words found in the Wikipedia dataset.  We'll use `Word2Vec` by passing in a `DataFrame` containing sentences.  We can pass into `Word2Vec` what length of vector to create, with larger vectors taking more time to build.\n",
    " \n",
    "Be able to convert words into vectors provides us with features that can be used in traditional machine learning algorithms.  These vectors can be used to compare word similarity, sentence similarity, or even larger sections of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseDir = '/mnt/ml-class/'\n",
    "dfSmall = sqlContext.read.parquet(baseDir + 'smallwiki.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfSmall.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out unwanted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import col\n",
    "filtered = dfSmall.filter((col('title') != '<PARSE ERROR>') &\n",
    "                           col('redirect_title').isNull() &\n",
    "                           col('text').isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change all text to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lowered = filtered.select('*', func.lower(col('text')).alias('lowerText'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed = (lowered\n",
    "          .drop('text')\n",
    "          .withColumnRenamed('lowerText', 'text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Wikipedia text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = r'(\\. |\\n{2,})'\n",
    "import re\n",
    "matches = re.findall(pattern, 'Wiki page. *More information*\\n\\n And a line\\n that continues.')\n",
    "print matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol='text', outputCol='sentences', pattern=pattern)\n",
    "sentences = tokenizer.transform(parsed).select('sentences')\n",
    "display(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "sentenceRDD = (sentences\n",
    "               .flatMap(lambda r: r[0])\n",
    "               .map(lambda x: Row(sentence=x)))\n",
    "\n",
    "sentenceSchema = StructType([StructField('sentence', StringType())])\n",
    "sentence = sqlContext.createDataFrame(sentenceRDD, sentenceSchema)\n",
    "\n",
    "display(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizerWord = RegexTokenizer(inputCol='sentence', outputCol='words', pattern=r'\\W+')\n",
    "words = tokenizerWord.transform(sentence).select('words')\n",
    "display(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our `removeWords` function that we registered in wiki-eda to clean up stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql('drop table if exists words')\n",
    "words.registerTempTable('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noStopWords = sqlContext.sql('select removeWords(words) as words from words') #.cache()\n",
    "display(noStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordVecInput = noStopWords.filter(func.size('words') != 0)\n",
    "wordVecInput.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the `Word2Vec` model.  This take about a minute with two workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=150, minCount=50, inputCol='words', outputCol='result', seed=0)\n",
    "model = word2Vec.fit(wordVecInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.findSynonyms('house', 10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "synonyms = model.findSynonyms('fruit', 10).collect()\n",
    "\n",
    "for word, similarity in synonyms:\n",
    "    print(\"{}: {}\".format(word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.findSynonyms('soccer', 10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we calculate similarity between vectors and handle creating a vector for multiple words at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "tmpDF = sqlContext.createDataFrame([Row(words=['fruit']),\n",
    "                                    Row(words=['flower']),\n",
    "                                    Row(words=['fruit', 'flower'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vFruit = model.transform(tmpDF).map(lambda r: r.result).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a cosine similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def similarity(x, y):\n",
    "  return x.dot(y) / (norm(x) * norm(y))\n",
    "\n",
    "print similarity(*vFruit[:2])\n",
    "print similarity(*vFruit[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Word2Vec` handles multiple words by averaging the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vFruit[0][:6]\n",
    "print vFruit[1][:6]\n",
    "print (vFruit[0][:6] + vFruit[1][:6]) / 2  # Averaging the word vectors gives us the vector for both words in a sentence\n",
    "print vFruit[2][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "tmpDF = sqlContext.createDataFrame([Row(words=['king']),\n",
    "                                    Row(words=['man']),\n",
    "                                    Row(words=['woman']),\n",
    "                                    Row(words=['queen'])])\n",
    "\n",
    "v1 = model.transform(tmpDF).rdd.map(lambda r: r.result).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k, m, w, q = v1\n",
    "print similarity(k, q)\n",
    "print similarity(k, (q + m)/2)\n",
    "print similarity(k, m)\n",
    "print similarity(q, m)\n",
    "print similarity(q, k - m + w)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
