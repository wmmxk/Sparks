{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia: Exploratory Data Analysis (EDA) using DataFrames\n",
    " \n",
    "This lab explores English wikipedia articles using `DataFrames`.  You'll learn about `DataFrame`, `Column`, and `GroupedData` objects and the `functions` package.  After you complete this lab you should be able to use much of the functionality found in Spark SQL and know where to find additional reference material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data and start the EDA\n",
    " \n",
    "We'll be mostly using functions and objects that are found in Spark SQL.  The [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package) APIs and the [Spark SQL and DataFrame Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html) are all very useful references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, work from the small sample to speed the EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseDir = '/mnt/ml-class/'\n",
    "dfSmall = sqlContext.read.parquet(baseDir + 'smallwiki.parquet').cache()\n",
    "print dfSmall.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how our `DataFrame` is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'dfSmall: {0}'.format(dfSmall)\n",
    "print '\\ntype(dfSmall): {0}'.format(type(dfSmall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dfSmall.schema, '\\n'\n",
    "dfSmall.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our schema is made up of a `StructType` that contains `StructField` objects.  These `StructField` objects have several properties including: name, data type, whether they can be null, and metadata.  Note that the list of fields for a `StructType` can also include other `StructType` objects to allow for nested structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField\n",
    "help(StructField)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an example `DataFrame` where we specify the schema using `StructType` and `StructField`.  Schema can also be inferred by Spark during `DataFrame` creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, LongType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([StructField('title', StringType(), nullable=False, metadata={'language': 'english'}),\n",
    "                     StructField('numberOfEdits', LongType()),\n",
    "                     StructField('redacted', BooleanType())])\n",
    "\n",
    "exampleData = sc.parallelize([Row(\"Baade's Window\", 100, False),\n",
    "                              Row('Zenomia', 10, True),\n",
    "                              Row('United States Bureau of Mines', 5280, True)])\n",
    "\n",
    "exampleDF = sqlContext.createDataFrame(exampleData, schema)\n",
    "display(exampleDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the schema that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print exampleDF.schema\n",
    "exampleDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `metadata` for the `title` field has also been captured.  We might create a new `DataFrame` from this `DataFrame` using a transformer and we could pass along or modify this `metadata` in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print exampleDF.schema.fields[0].metadata\n",
    "print exampleDF.schema.fields[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a row of wikipedia data look like?  Let's take a look at the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dfSmall.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are our column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dfSmall.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is long and obscures the rest of the data.  Let's use `drop` to remove the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dfSmall.drop('text').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's view the text in a format that more closely resembles how it would be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dfSmall.select('text').first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we parsed the XML we stored `<PARSE ERROR>` as the title for any record that our XML parser couldn't handle.  Let's see how many records had errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "errors = dfSmall.filter(col('title') == '<PARSE ERROR>')\n",
    "errorCount = errors.count()\n",
    "print errorCount\n",
    "print errorCount / float(dfSmall.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the `Column` selection several different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dfSmall.filter(dfSmall['title'] == '<PARSE ERROR>').count()\n",
    "print dfSmall.filter(dfSmall.title == '<PARSE ERROR>').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `errors` contains those items with a title that equals `<PARSE ERROR>`.  Note that we can rename our column using `.alias()` and display our `DataFrame` using `.show()`.  `alias` is a method that we are calling on a `Column` and `show` is a method called on the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors.select(col('title').alias('badTitle')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what does an error look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print errors.select('text').first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some `Column` and `DataFrame` operations to inspect the `redirect_title` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(dfSmall\n",
    " .select(col('redirect_title').isNotNull().alias('hasRedirect'))\n",
    " .groupBy('hasRedirect')\n",
    " .count()\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's filter out the data that has a parse error, is a redirect, or doesn't have any text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = dfSmall.filter((col('title') != '<PARSE ERROR>') &\n",
    "                           col('redirect_title').isNull() &\n",
    "                           col('text').isNotNull())\n",
    "print filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful functions\n",
    " \n",
    "In addition to the functions that can be called on a `DataFrame`, `Column`, or `GroupedData`, Spark SQL also has a `functions` package that provides functions like those typically built into a database system that can be called from SQL.  This include functions for performing mathematical operations, handling dates and times, string manipulation, and more.\n",
    " \n",
    "The [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) APIs have good descriptions for these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the time functions to convert our timestamp into Central European Summer Time (CEST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered.select('timestamp').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try applying `date_format` to see how it operates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(filtered\n",
    " .select('timestamp', func.date_format('timestamp', 'MM/dd/yyyy').alias('date'))\n",
    " .show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "withDate = filtered.withColumn('date', func.date_format('timestamp', 'MM/dd/yyyy'))\n",
    "withDate.printSchema()\n",
    "withDate.select('title', 'timestamp', 'date').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we want a different function for time zone manipulation and to store the object as a timestamp rather than a string.  Let's use `from_utc_timestamp` to get a timestamp object back with the correct time zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "withCEST = withDate.withColumn('cest_time', func.from_utc_timestamp('timestamp', 'Europe/Amsterdam'))\n",
    "withCEST.printSchema()\n",
    "\n",
    "(withCEST\n",
    " .select('timestamp', 'cest_time')\n",
    " .show(3, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's convert the text field to lowercase.  We'll use the `lower` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lowered = withCEST.select('*', func.lower(col('text')).alias('lowerText'))\n",
    "\n",
    "print lowered.select('lowerText').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What columns do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lowered.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and drop the columns we don't want and rename `lowerText` to `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed = (lowered\n",
    "          .drop('text')\n",
    "          .drop('timestamp')\n",
    "          .drop('date')\n",
    "          .withColumnRenamed('lowerText', 'text'))\n",
    "\n",
    "print parsed.columns, '\\n\\n'\n",
    "print parsed.select('text').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's convert our text into a list of words so that we can perform some analysis at the word level.  For this we will use a feature transformer called `RegexTokenizer` which splits up strings into tokens (words in our case) based on a split pattern.  We'll split our text on anything that matches one or more non-word characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "tokenizer = (RegexTokenizer()\n",
    "             .setInputCol('text')\n",
    "             .setOutputCol('words')\n",
    "             .setPattern('\\\\W+'))\n",
    "wordsDF = tokenizer.transform(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsDF.select('words').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some very common words in our list of words which won't be that useful for our later analysis.  We'll create a UDF to remove them.\n",
    " \n",
    "[StopWordsRemover](http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover) is implemented for Scala but not yet for Python.  We'll use the same [list](http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words) of stop words it uses to build a user-defined function (UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopWords = set(sc.textFile('/mnt/ml-class/stop_words.txt').collect())\n",
    "print [word for i, word in zip(range(5), stopWords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our function for removing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "stopWordsBroadcast = sc.broadcast(stopWords)\n",
    "\n",
    "def keepWord(word):\n",
    "    if len(word) < 3:\n",
    "        return False\n",
    "\n",
    "    if word in stopWordsBroadcast.value:\n",
    "        return False\n",
    "\n",
    "    if re.search(re.compile(r'[0-9_]'), word):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def removeWords(words):\n",
    "    return [word for word in words if keepWord(word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "removeWords(['test', 'cat', 'do343', '343', 'spark', 'the', 'and', 'hy-phen', 'under_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a UDF from our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "removeWordsUDF = udf(removeWords, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register this function so that we can call it later from another notebook.  Note that in Scala `register` also returns a `udf` that we can use, so we could have combined the above step into this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.udf.register('removeWords', removeWords, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply our function to the `wordsDF` `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noStopWords = (wordsDF\n",
    "               .withColumn('noStopWords', removeWordsUDF(col('words')))\n",
    "               .drop('words')\n",
    "               .withColumnRenamed('noStopWords', 'words'))\n",
    "\n",
    "noStopWords.select('words').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save our work at this point by writing out a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#noStopWords.write.parquet(\"/mnt/ml-class/smallWords.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the `DataFrame` doing in the background?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print noStopWords.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cache `noStopWords` as we'll use it multiple times shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noStopWords.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of words in `noStopWords`.  Recall that each row contains an array of words.\n",
    " \n",
    "One strategy would be to take the length of each row and sum the lengths.  To do this use `functions.size`, `functions.sum`, and call `.agg` on the `DataFrame`.\n",
    " \n",
    "Don't forget to refer to the  [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package) APIs.  For example you'll find detail for the function `size` in the [functions module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.size) in Python and the [functions package](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) in Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a `DataFrame` named sized that has a `size` column with the size of each array of words.  Here you can use `func.size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "sized = noStopWords.withColumn('size', func.size('words'))\n",
    "\n",
    "sizedFirst = sized.select('size', 'words').first()\n",
    "print sizedFirst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "from test_helper import Test\n",
    "Test.assertEquals(sizedFirst[0], len(sizedFirst[1]), 'incorrect implementation for sized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll need to aggregate the counts.  You can do this using `func.sum` in either a `.select` or `.agg` method call on the `DataFrame`.  Make sure to give your `Column` the alias `numberOfWords`.  There are some examples in [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) in the APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "numberOfWords = sized.agg(func.sum('size').alias('numberOfWords'))\n",
    "\n",
    "wordCount = numberOfWords.first()[0]\n",
    "print wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Test.assertEquals(wordCount, 1903220, 'incorrect word count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll compute the word count using `select`, the function `func.explode()`, and then taking a `count()` on the `DataFrame`.  Make sure to name the column returned by the `explode` function 'word'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "wordList = noStopWords.select(func.explode('words').alias('word'))\n",
    "\n",
    "# Note that we have one word per Row now\n",
    "print wordList.take(3)\n",
    "wordListCount = wordList.count()\n",
    "print wordListCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Test.assertEquals(wordListCount, 1903220, 'incorrect value for wordListCount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your final task, you'll group by word and count the number of times each word occurs.  Make sure to return the counts in descending order and to call them `counts`.\n",
    " \n",
    "For this task, you can use:\n",
    " * `DataFrame` operations `groupBy`, `agg`, and `sort`\n",
    " * the `Column` operation `alias`\n",
    " * functions `func.count` and `func.desc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "wordGroupCount = (wordList\n",
    "                  .groupBy('word')  # group\n",
    "                  .agg(func.count('word').alias('counts'))  # aggregate\n",
    "                  .sort(func.desc('counts')))  #sort\n",
    "\n",
    "wordGroupCount.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Test.assertEquals(tuple(wordGroupCount.first()), (u'ref', 29263), 'incorrect counts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use SQL to accomplish this counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordList.registerTempTable('wordList')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordGroupCount2 = sqlContext.sql('select word, count(word) as counts from wordList group by word order by counts desc')\n",
    "wordGroupCount2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select word, count(word) as counts from wordList group by word order by counts desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how many distinct words we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinctWords = wordList.distinct()\n",
    "distinctWords.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinctWords.count()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
