{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines and Logistic Regression\n",
    " \n",
    "In this lab we'll cover transformers, estimators, evaluators, and pipelines.  We'll use transformers and estimators to prepare our data for use in a logistic regression model and will use pipelines to combine these steps together.  Finally, we'll evaluate our model.\n",
    " \n",
    "This lab also covers creating train and test datasets using `randomSplit`, visualizing a ROC curve, and generating both `ml` and `mllib` logistic regression models.\n",
    " \n",
    "After completing this lab you should be comfortable using transformers, estimators, evaluators, and pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseDir = \"/mnt/ml-class/\"\n",
    "irisTwoFeatures = sqlContext.read.parquet(baseDir + 'irisTwoFeatures.parquet').cache()\n",
    "print '\\n'.join(map(repr, irisTwoFeatures.take(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(irisTwoFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data\n",
    " \n",
    "To explore our data in more detail, we're going to we pull out sepal length and sepal width and create two columns.  These are the two features found in our `DenseVector`.\n",
    " \n",
    "In order to do this you will write a `udf` that takes in two values.  The first will be the name of the vector that we are operating on and the second is a literal for the index position.  Here are links to `lit` in the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.lit) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) APIs.\n",
    " \n",
    "The `udf` will return a `DoubleType` that is the value of the specified vector at the specified index position.\n",
    " \n",
    "In order to call our function, we need to wrap the second value in `lit()` (e.g. `lit(1)` for the second element).  This is because our `udf` expects a `Column` and `lit` generates a `Column` where the literal is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Remember to cast the value you extract from the Vector using float()\n",
    "getElement = udf(lambda v, i: float(v[i]), DoubleType())\n",
    "\n",
    "irisSeparateFeatures = (irisTwoFeatures\n",
    "                        .withColumn('sepalLength', getElement('features', lit(0)))\n",
    "                        .withColumn('sepalWidth', getElement('features', lit(1))))\n",
    "display(irisSeparateFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "from test_helper import Test\n",
    "firstRow = irisSeparateFeatures.select('sepalWidth', 'features').map(lambda r: (r[0], r[1])).first()\n",
    "Test.assertEquals(firstRow[0], firstRow[1][1], 'incorrect definition for getElement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about using `Column`'s `getItem` method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "try:\n",
    "    display(irisTwoFeatures.withColumn('sepalLength', col('features').getItem(0)))\n",
    "except AnalysisException as e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it doesn't work for vectors, but it does work on arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "arrayDF = sqlContext.createDataFrame([Row(anArray=[1,2,3]), Row(anArray=[4,5,6])])\n",
    "arrayDF.show()\n",
    "\n",
    "arrayDF.select(col('anArray').getItem(0)).show()\n",
    "arrayDF.select(col('anArray')[1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's register our function and then call it directly from SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.udf.register('getElement', getElement.func, getElement.returnType)\n",
    "irisTwoFeatures.registerTempTable('irisTwo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select getElement(features, 0) as sepalLength from irisTwo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the ranges of our values and view their means and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(irisSeparateFeatures.describe('label', 'sepalLength', 'sepalWidth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features both take on values from -1.0 to 1.0, but have different means and standard deviations.  How could we standardize our data to have zero mean and unit standard deviations?  For this task we'll use the `ml` estimator `StandardScaler`.  Feature transformers (which are sometimes estimators) can be found in [pyspark.ml.feature](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature) for Python or [org.apache.spark.ml.feature](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.package) for Scala.\n",
    " \n",
    "Also, remember that the [ML Guide](http://spark.apache.org/docs/latest/ml-features.html#standardscaler) is a good place to find additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "standardScaler = (StandardScaler()\n",
    "                  .setInputCol('features')\n",
    "                  .setOutputCol('standardized')\n",
    "                  .setWithMean(True))\n",
    "\n",
    "print standardScaler.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisStandardizedLength = (standardScaler\n",
    "                          .fit(irisSeparateFeatures)\n",
    "                          .transform(irisSeparateFeatures)\n",
    "                          .withColumn('standardizedLength', getElement('standardized', lit(0))))\n",
    "display(irisStandardizedLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(irisStandardizedLength.describe('sepalLength', 'standardizedLength'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if instead we wanted to normalize the data?  For example, we might want to normalize each set of features (per row) to have length one using an \\\\( l^2 \\\\) norm.  That would cause the sum of the features squared to be one: \\\\( \\sum_{i=1}^d x_i^2 = 1 \\\\).  This is could be useful if we wanted to compare observations based on a distance metric like in k-means clustering.\n",
    " \n",
    "`Normalizer` can be found in [pyspark.ml.feature](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Normalizer) for Python and the [org.apache.spark.ml.feature](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.Normalizer) package for Scala.\n",
    " \n",
    "Let's implement `Normalizer` and transform our features.  Make sure to use a `P` of 2.0 and to name the output column \"featureNorm\".  Remember that we're working with the `irisTwoFeatures` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml.feature import Normalizer\n",
    "normalizer = (Normalizer()\n",
    "              .setInputCol('features')\n",
    "              .setOutputCol('featureNorm')\n",
    "              .setP(2.0))\n",
    "\n",
    "irisNormalized = normalizer.transform(irisTwoFeatures)  # Note that we're calling transform here\n",
    "display(irisNormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "import numpy as np\n",
    "firstVector = irisNormalized.select('featureNorm').map(lambda r: r[0]).first()\n",
    "Test.assertTrue(np.allclose(firstVector.norm(2.0), 1.0), 'incorrect setup of normalizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just check and see that our norms are equal to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l2Norm = udf(lambda v: float(v.norm(2.0)), DoubleType())\n",
    "\n",
    "featureLengths = irisNormalized.select(l2Norm('features').alias('featuresLength'),\n",
    "                                       l2Norm('featureNorm').alias('featureNormLength'))\n",
    "display(featureLengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's bucketize our features.  This will allow us to convert continuous features into discrete buckets.  This is often desirable for logistic regression which we'll be performing later in this lab.\n",
    " \n",
    "We'll use the following splits: -infinity, -0.5, 0.0, 0.5, +infinity.  Note that in Python infinity can be represented using `float('inf')` and that in Scala `Double.NegativeInfinity` and `Double.PositiveInfinity` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float('inf'), -.5, 0.0, .5, float('inf')]\n",
    "\n",
    "lengthBucketizer = (Bucketizer()\n",
    "              .setInputCol('sepalLength')\n",
    "              .setOutputCol('lengthFeatures')\n",
    "              .setSplits(splits))\n",
    "\n",
    "irisBucketizedLength = lengthBucketizer.transform(irisSeparateFeatures)\n",
    "display(irisBucketizedLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "widthBucketizer = (Bucketizer()\n",
    "                   .setInputCol(\"sepalWidth\")\n",
    "                   .setOutputCol(\"widthFeatures\")\n",
    "                   .setSplits(splits))\n",
    "\n",
    "irisBucketizedWidth = widthBucketizer.transform(irisBucketizedLength)\n",
    "display(irisBucketizedWidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the two bucketizers into a [Pipeline](http://spark.apache.org/docs/latest/ml-guide.html#pipeline-components) that performs both bucketizations.  A `Pipeline` is made up of stages which can be set using `setStages` and passing in a `list` of stages in Python or an `Array` of stages in Scala.  `Pipeline` is an estimator, which means it implements a `fit` method which returns a `PipelineModel`.  A `PipelineModel` is a transformer, which means that it implements a `transform` method which can be used to run the stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "pipelineBucketizer = Pipeline().setStages([lengthBucketizer, widthBucketizer])\n",
    "\n",
    "pipelineModelBucketizer = pipelineBucketizer.fit(irisSeparateFeatures)\n",
    "irisBucketized = pipelineModelBucketizer.transform(irisSeparateFeatures)\n",
    "\n",
    "display(irisBucketized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created two new features through bucketing, let's combine those two features into a `Vector` with `VectorAssembler`.  VectorAssembler can be found in [pyspark.ml.feature](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) for Python and the [org.apache.spark.ml.feature](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.VectorAssembler) package for Scala.\n",
    " \n",
    "Set the params of `assembler` so that both \"lengthFeatures\" and \"widthFeatures\" are assembled into a column called \"featuresBucketized\".\n",
    " \n",
    "Then, set the stages of `pipeline` to include both bucketizers and the assembler as the last stage.\n",
    " \n",
    "Finally, use `pipeline` to generate a new `DataFrame` called `irisAssembled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "pipeline = Pipeline()\n",
    "assembler = VectorAssembler()\n",
    "\n",
    "print assembler.explainParams()\n",
    "print '\\n',pipeline.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# Set assembler params\n",
    "(assembler\n",
    " .setInputCols(['lengthFeatures', 'widthFeatures'])\n",
    " .setOutputCol('featuresBucketized'))\n",
    "\n",
    "pipeline.setStages([lengthBucketizer, widthBucketizer, assembler])\n",
    "irisAssembled = pipeline.fit(irisSeparateFeatures).transform(irisSeparateFeatures)\n",
    "display(irisAssembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "firstAssembly = irisAssembled.select('lengthFeatures', 'widthFeatures', 'featuresBucketized').first()\n",
    "Test.assertTrue(all(firstAssembly[2].toArray() == [firstAssembly[0], firstAssembly[1]]),\n",
    "                'incorrect value for column featuresBucketized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at our data by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(irisSeparateFeatures.groupBy('label').count().orderBy('label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a model that tries to differentiate between the first two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "irisTwoClass = irisSeparateFeatures.filter(col('label') < 2)\n",
    "display(irisTwoClass.groupBy('label').count().orderBy('label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll split our dataset into test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisTest, irisTrain = irisTwoClass.randomSplit([.25, .75], seed=0)\n",
    "\n",
    "# Cache as we'll be using these several times\n",
    "irisTest.cache()\n",
    "irisTrain.cache()\n",
    "\n",
    "print 'Items in test datset: {0}'.format(irisTest.count())\n",
    "print 'Items in train dataset: {0}'.format(irisTrain.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's build our logistic regression model.  LogisticRegression can be found in [pyspark.ml.classification](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression) for Python and the [org.apache.spark.ml.classification](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.classification.LogisticRegression) package for Scala.  The ML Guide also has a nice overview of [logistic regression](http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression).\n",
    " \n",
    "Make sure to set the featuresCol to \"featuresBucketized\", the regParam to 0.0, the labelCol to \"label\", and the maxIter to 1000.\n",
    " \n",
    "Also, set the pipeline stages to include the two bucketizers, assembler, and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = (LogisticRegression()\n",
    "      .setFeaturesCol('featuresBucketized')\n",
    "      .setRegParam(0.0)\n",
    "      .setLabelCol('label')\n",
    "      .setMaxIter(1000))\n",
    "\n",
    "pipeline.setStages([lengthBucketizer, widthBucketizer, assembler, lr])\n",
    "\n",
    "pipelineModelLR = pipeline.fit(irisTrain)\n",
    "\n",
    "irisTestPredictions = (pipelineModelLR\n",
    "                       .transform(irisTest)\n",
    "                       .cache())\n",
    "display(irisTestPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisTestPredictions.select(\"probability\").first()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Test.assertTrue(sum(irisTestPredictions.select(\"probability\").first()[0]) > .99,\n",
    "                'incorrect build of the lr model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pipelineModelLR.stages\n",
    "print '\\n{0}'.format(pipelineModelLR.stages[-1].weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaving our features to range from 0 to 3 means that a value of 2 has twice the impact in our model than a value of 1.  Since these buckets were based on increasing numeric values this is not unreasonable; however, we might want to convert each of these values to a dummy feature that takes on either a 0 or 1 corresponding to whether the value occurs.  This allows the model to measure the impact of the occurrences of the individual values and allows for non-linear relationships.\n",
    " \n",
    "To do this we'll use the `OneHotEncoder` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "oneHotLength = (OneHotEncoder()\n",
    "                .setInputCol('lengthFeatures')\n",
    "                .setOutputCol('lengthOneHot'))\n",
    "\n",
    "pipeline.setStages([lengthBucketizer, widthBucketizer, oneHotLength])\n",
    "\n",
    "irisWithOneHotLength = pipeline.fit(irisTrain).transform(irisTrain)\n",
    "display(irisWithOneHotLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisWithOneHotLength.select('lengthOneHot').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `OneHotEncoder` for width as well, and combine both encoders together into a `featuresBucketized` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oneHotWidth = (OneHotEncoder()\n",
    "               .setInputCol('widthFeatures')\n",
    "               .setOutputCol('widthOneHot'))\n",
    "\n",
    "assembleOneHot = (VectorAssembler()\n",
    "                  .setInputCols(['lengthOneHot', 'widthOneHot'])\n",
    "                  .setOutputCol('featuresBucketized'))\n",
    "\n",
    "pipeline.setStages([lengthBucketizer, widthBucketizer, oneHotLength, oneHotWidth, assembleOneHot])\n",
    "\n",
    "display(pipeline.fit(irisTrain).transform(irisTrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the full `Pipeline` through logistic regression and make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.setStages([lengthBucketizer, widthBucketizer, oneHotLength, oneHotWidth, assembleOneHot, lr])\n",
    "\n",
    "pipelineModelLR2 = pipeline.fit(irisTrain)\n",
    "\n",
    "irisTestPredictions2 = (pipelineModelLR2\n",
    "                        .transform(irisTest)\n",
    "                        .cache())\n",
    "display(irisTestPredictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our new model look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logisticModel = pipelineModelLR2.stages[-1]\n",
    "print logisticModel.intercept\n",
    "print repr(logisticModel.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about model accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def modelAccuracy(df):\n",
    "  return (df\n",
    "          .select((col('prediction') == col('label')).cast('int').alias('correct'))\n",
    "          .groupBy()\n",
    "          .avg('correct')\n",
    "          .first()[0])\n",
    "\n",
    "modelOneAccuracy = modelAccuracy(irisTestPredictions)\n",
    "modelTwoAccuracy = modelAccuracy(irisTestPredictions2)\n",
    "\n",
    "print 'modelOneAccuracy: {0:.3f}'.format(modelOneAccuracy)\n",
    "print 'modelTwoAccuracy: {0:.3f}'.format(modelTwoAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use SQL instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisTestPredictions.registerTempTable('modelOnePredictions')\n",
    "sqlResult = sqlContext.sql('select avg(int(prediction == label)) from modelOnePredictions')\n",
    "display(sqlResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even better option is to use the tools already built-in to Spark.  The MLlib guide has a lot of information regarding [evaluation metrics](http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html).  For ML, you can find details in the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.evaluation) and [Scala](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.evaluation.package) APIs.\n",
    " \n",
    "A common metric used for logistic regression is area under the ROC curve (AUC).  We can use the `BinaryClasssificationEvaluator` to obtain the AUC for our two models.  Make sure to set the metric to \"areaUnderROC\" and that you set the rawPrediction column to \"rawPrediction\".\n",
    " \n",
    "Recall that `irisTestPredictions` are the test predictions from our first model and `irisTestPredictions2` are the test predictions from our second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "binaryEvaluator = (BinaryClassificationEvaluator()\n",
    "                   .setRawPredictionCol('rawPrediction')\n",
    "                   .setMetricName('areaUnderROC'))\n",
    "\n",
    "firstModelTestAUC = binaryEvaluator.evaluate(irisTestPredictions)\n",
    "secondModelTestAUC = binaryEvaluator.evaluate(irisTestPredictions2)\n",
    "\n",
    "print 'First model AUC: {0:.3f}'.format(firstModelTestAUC)\n",
    "print 'Second model AUC: {0:.3f}'.format(secondModelTestAUC)\n",
    "\n",
    "irisTrainPredictions = pipelineModelLR.transform(irisTrain)\n",
    "irisTrainPredictions2 = pipelineModelLR2.transform(irisTrain)\n",
    "\n",
    "firstModelTrainAUC = binaryEvaluator.evaluate(irisTrainPredictions)\n",
    "secondModelTrainAUC = binaryEvaluator.evaluate(irisTrainPredictions2)\n",
    "\n",
    "print '\\nFirst model training AUC: {0:.3f}'.format(firstModelTrainAUC)\n",
    "print 'Second model training AUC: {0:.3f}'.format(secondModelTrainAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Test.assertTrue(firstModelTestAUC > .95, 'incorrect firstModelTestAUC')\n",
    "Test.assertTrue(secondModelTrainAUC > .95, 'incorrect secondModelTrainAUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization: ROC curve **\n",
    " \n",
    "We will now visualize how well the model predicts our target.  To do this we generate a plot of the ROC curve.  The ROC curve shows us the trade-off between the false positive rate and true positive rate, as we liberalize the threshold required to predict a positive outcome.  A random model is represented by the dashed line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "def prepareSubplot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0, subplots=(1, 1)):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, axList = plt.subplots(subplots[0], subplots[1], figsize=figsize, facecolor='white',\n",
    "                               edgecolor='white')\n",
    "    if not isinstance(axList, np.ndarray):\n",
    "        axList = np.array([axList])\n",
    "\n",
    "    for ax in axList.flatten():\n",
    "        ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "        for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "            axis.set_ticks_position('none')\n",
    "            axis.set_ticks(ticks)\n",
    "            axis.label.set_color('#999999')\n",
    "            if hideLabels: axis.set_ticklabels([])\n",
    "        ax.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "        map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "\n",
    "    if axList.size == 1:\n",
    "        axList = axList[0]  # Just return a single axes object for a regular plot\n",
    "    return fig, axList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateROC(ax, labelsAndScores):\n",
    "    labelsAndWeights = labelsAndScores.collect()\n",
    "    labelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\n",
    "    labelsByWeight = np.array([0.0] + [k for (k, v) in labelsAndWeights])\n",
    "\n",
    "    length = labelsByWeight.size - 1\n",
    "    truePositives = labelsByWeight.cumsum()\n",
    "    numPositive = truePositives[-1]\n",
    "    falsePositives = np.arange(0.0, length + 1, 1.) - truePositives\n",
    "\n",
    "    truePositiveRate = truePositives / numPositive\n",
    "    falsePositiveRate = falsePositives / (length - numPositive)\n",
    "\n",
    "    # Generate layout and plot data\n",
    "    ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "    ax.plot(falsePositiveRate, truePositiveRate, color='red', linestyle='-', linewidth=3.)\n",
    "    ax.plot((0., 1.), (0., 1.), linestyle='--', color='orange', linewidth=2.)  # Baseline model\n",
    "\n",
    "\n",
    "labelsAndScores = (irisTestPredictions\n",
    "                   .select('label', 'rawPrediction')\n",
    "                   .rdd\n",
    "                   .map(lambda r: (r[0], r[1][1])))\n",
    "\n",
    "labelsAndScores2 = (irisTestPredictions2\n",
    "                    .select('label', 'rawPrediction')\n",
    "                    .rdd\n",
    "                    .map(lambda r: (r[0], r[1][1])))\n",
    "\n",
    "fig, axList = prepareSubplot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1), figsize=(12., 5.), subplots=(1,2))\n",
    "ax0, ax1 = axList\n",
    "ax0.set_title('First Model', color='#999999')\n",
    "ax1.set_title('Second Model', color='#999999')\n",
    "generateROC(axList[0], labelsAndScores)\n",
    "generateROC(axList[1], labelsAndScores2)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "metric = 'precision'\n",
    "\n",
    "multiclassEval = MulticlassClassificationEvaluator()\n",
    "\n",
    "multiclassEval.setMetricName(metric)\n",
    "print 'Model one {0}: {1:.3f}'.format(metric, multiclassEval.evaluate(irisTestPredictions))\n",
    "print 'Model two {0}: {1:.3f}\\n'.format(metric, multiclassEval.evaluate(irisTestPredictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "print inspect.getsource(MulticlassClassificationEvaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using MLlib instead of ML\n",
    " \n",
    "We've been using `ml` transformers, estimators, pipelines, and evaluators.  How can we accomplish the same things with MLlib?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisTestPredictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisTestPredictions.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the data that we need from our `DataFrame` and create `BinaryClassificationMetrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "modelOnePredictionLabel = (irisTestPredictions\n",
    "                           .select('rawPrediction', 'label')\n",
    "                           .rdd\n",
    "                           .map(lambda r: (float(r[0][1]), r[1])))\n",
    "\n",
    "modelTwoPredictionLabel = (irisTestPredictions2\n",
    "                           .select('rawPrediction', 'label')\n",
    "                           .rdd\n",
    "                           .map(lambda r: (float(r[0][1]), r[1])))\n",
    "\n",
    "metricsOne = BinaryClassificationMetrics(modelOnePredictionLabel)\n",
    "metricsTwo = BinaryClassificationMetrics(modelTwoPredictionLabel)\n",
    "\n",
    "print metricsOne.areaUnderROC\n",
    "print metricsTwo.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a logistic regression model with MLlib we'll need the data to be an RDD of `LabeledPoints`.  For testing purposes we'll pull out the label and features into a tuple, since we'll want to make predictions directly on the features and not on a `LabeledPoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "irisTrainRDD = (irisTrainPredictions\n",
    "                .select('label', 'featuresBucketized')\n",
    "                .map(lambda r: LabeledPoint(r[0], r[1]))\n",
    "                .cache())\n",
    "\n",
    "irisTestRDD = (irisTestPredictions\n",
    "               .select('label', 'featuresBucketized')\n",
    "               .map(lambda r: (r[0], r[1]))\n",
    "               .cache())\n",
    "\n",
    "print irisTrainRDD.take(2)\n",
    "print irisTestRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use MLlib's logistic regression on our `RDD` of `LabeledPoints`.  Note that we'll use `LogisticRegressionWithLBFGS` as it tends to converge faster than `LogisticRegressionWithSGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "help(LogisticRegressionWithLBFGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mllibModel = LogisticRegressionWithLBFGS.train(irisTrainRDD, iterations=1000, regParam=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate our accuracy using `RDDs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rddPredictions = mllibModel.predict(irisTestRDD.values())\n",
    "predictAndLabels = rddPredictions.zip(irisTestRDD.keys())\n",
    "\n",
    "mllibAccuracy = predictAndLabels.map(lambda (p, l): p == l).mean()\n",
    "print 'MLlib model accuracy: {0:.3f}'.format(mllibAccuracy)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
