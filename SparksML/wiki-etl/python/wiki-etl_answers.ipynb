{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia: ETL\n",
    " \n",
    "This lab explains the process that was used to obtain Wikipedia data and transform it into a more usable form for analysis in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL Background\n",
    " \n",
    "Wikipedia data from the [August 5, 2015](https://dumps.wikimedia.org/enwiki/20150805/) enwiki dump on dumps.wikimedia.org.  Using the file: `enwiki-20150805-pages-articles-multistream.xml.bz2`  The file was uncompressed, parsed to pull out the XML `<page>` tags, and parsed again to retrieve several fields as JSON.  The JSON was stored one JSON string per line so that Spark could easily load the JSON and write out a parquet file.  The resulting parquet file was downsampled to keep the dataset small for the labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikiSample = \"\"\"<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\n",
    "  <siteinfo>\n",
    "    <sitename>Wikipedia</sitename>\n",
    "    <dbname>enwiki</dbname>\n",
    "    <base>https://en.wikipedia.org/wiki/Main_Page</base>\n",
    "    <generator>MediaWiki 1.26wmf16</generator>\n",
    "    <case>first-letter</case>\n",
    "    <namespaces>\n",
    "      <namespace key=\"-2\" case=\"first-letter\">Media</namespace>\n",
    "      <namespace key=\"-1\" case=\"first-letter\">Special</namespace>\n",
    "      ...\n",
    "  <page>\n",
    "    <title>AccessibleComputing</title>\n",
    "    <ns>0</ns>\n",
    "    <id>10</id>\n",
    "    <redirect title=\"Computer accessibility\" />\n",
    "    <revision>\n",
    "      <id>631144794</id>\n",
    "      <parentid>381202555</parentid>\n",
    "      <timestamp>2014-10-26T04:50:23Z</timestamp>\n",
    "      <contributor>\n",
    "        <username>Paine Ellsworth</username>\n",
    "        <id>9092818</id>\n",
    "      </contributor>\n",
    "      <comment>add [[WP:RCAT|rcat]]s</comment>\n",
    "      <model>wikitext</model>\n",
    "      <format>text/x-wiki</format>\n",
    "      <text xml:space=\"preserve\">#REDIRECT [[Computer accessibility]]{{Redr|move|from CamelCase|up}}</text>\n",
    "      <sha1>4ro7vvppa5kmm0o1egfjztzcwd0vabw</sha1>\n",
    "    </revision>\n",
    "  </page>\n",
    "  ...\n",
    "</mediawiki>\"\"\"\n",
    "import cgi\n",
    "displayHTML('<pre>{0}</pre>'.format(cgi.escape(wikiSample, True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import ParseError\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def parse_xml_to_dict(xmlString):\n",
    "    data = {\"title\": None, \"redirect_title\": None, \"timestamp\": None, \"last_contributor_username\": None, \"text\": None}\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(xmlString.encode('utf-8'))\n",
    "\n",
    "        title = root.find(\"title\")\n",
    "        if title is not None:\n",
    "            data[\"title\"] = title.text\n",
    "        redirect = root.find(\"redirect\")\n",
    "        if redirect is not None:\n",
    "            data[\"redirect_title\"] = redirect.attrib[\"title\"]\n",
    "        revision = root.find(\"revision\")\n",
    "        if revision is not None:\n",
    "            timestamp = revision.find(\"timestamp\")\n",
    "            data[\"timestamp\"] = timestamp.text\n",
    "        contributor = revision.find(\"contributor\")\n",
    "        if contributor is not None:\n",
    "            username = contributor.find(\"username\")\n",
    "        if username is not None:\n",
    "            data[\"last_contributor_username\"] = username.text\n",
    "        text = revision.find(\"text\")\n",
    "        if text is not None and text.text is not None:\n",
    "            data[\"text\"] = text.text.replace(\"\\\\n\", \" \")\n",
    "    except ParseError:\n",
    "        data['title'] = '<PARSE ERROR>'\n",
    "\n",
    "    return data #Row(**dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from xml.etree.ElementTree import ParseError\n",
    "\n",
    "wikiData = codecs.open('/mnt/data_quick/wiki/extract/enwiki-20150805-pages-articles-multistream.xml', 'r', 'utf-8')\n",
    "jsonData = codecs.open('/mnt/data_quick/wiki/allpages.json', 'w', 'utf-8')\n",
    "\n",
    "pageData = []\n",
    "articleCount = 0\n",
    "pageCount = 0\n",
    "pageStart = 0\n",
    "\n",
    "for i, line in enumerate(wikiData):\n",
    "    #if i > 10000:\n",
    "    #    break\n",
    "\n",
    "    if '<page>' in line:\n",
    "        pageCount += 1\n",
    "\n",
    "        if pageCount > 1:\n",
    "            print 'unexpected to have pageCount > 1'\n",
    "        else:\n",
    "            articleCount += 1\n",
    "            pageStart = line.index('<page>')\n",
    "\n",
    "    if pageCount > 0:\n",
    "        pageData.append(line[pageStart:])\n",
    "\n",
    "    if '</page>' in line:\n",
    "        pageCount -= 1\n",
    "\n",
    "        if pageCount == 0:\n",
    "            try:\n",
    "                fromxml = parse_xml_to_row(u'\\n'.join(pageData))\n",
    "            except ParseError:\n",
    "                print u'\\n'.join(pageData)\n",
    "                break\n",
    "\n",
    "            json.dump(fromxml, jsonData)\n",
    "            jsonData.write('\\n')\n",
    "            pageData = []\n",
    "\n",
    "jsonData.close()\n",
    "wikiData.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a json file and use Spark to output a Parquet file.\n",
    " \n",
    "Parquet files store data by column in a compressed and efficient manner.  More details can be found at [parquet.apache.org](https://parquet.apache.org/documentation/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json(\"/mnt/data_quick/wiki/allpages.json\")\n",
    "df.write.parquet(\"/mnt/data_quick/wiki/allpages.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back in the data as `df2` so that the more efficient Parquet format will be our starting point for the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = sqlContext.read.parquet(\"/mnt/data_quick/wiki/allpages.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a 1% sample of the data as another Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfOne = df2.sample(False, .01, 2718).coalesce(24)\n",
    "dfOne.write.parquet('/mnt/data_quick/wiki/onepercent.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an even smaller sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfSmall = df2.sample(False, .0005, 2718).coalesce(8)\n",
    "dfSmall.write.parquet(\"/mnt/data_quick/wiki/smallwiki.parquet\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
