{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib Data Types\n",
    " \n",
    "This notebook explains the machine learning specific data types in Spark.  The focus is on the data types and classes used for generating models.  These include: `DenseVector`, `SparseVector`, `LabeledPoint`, and `Rating`.\n",
    " \n",
    "For reference:\n",
    " \n",
    "The [MLlib Guide](http://spark.apache.org/docs/latest/mllib-guide.html) provides an overview of all aspects of MLlib and [MLlib Guide: Data Types](http://spark.apache.org/docs/latest/mllib-data-types.html) provides a detailed review of data types specific for MLlib\n",
    " \n",
    "After this lab you should understand the differences between `DenseVectors` and `SparseVectors` and be able to create and use `DenseVector`, `SparseVector`, `LabeledPoint`, and `Rating` objects.  You'll also learn where to obtain additional information regarding the APIs and specific class / method functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib import linalg\n",
    "dir(linalg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(linalg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### Dense and Sparse\n",
    " \n",
    "MLlib supports both dense and sparse types for vectors and matrices.  We'll focus on vectors as they are most commonly used in MLlib and matrices have poor scaling properties.\n",
    " \n",
    "A dense vector contains an array of values, while a sparse vector stores the size of the vector, an array of indices, and an array of values that correspond to the indices.  A sparse vector saves space by not storing zero values.\n",
    " \n",
    "For example, if we had the dense vector `[2.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0]`, we could store that as a sparse vector with size 7, indices as `[0, 3]`, and values as `[2.0, 3.0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import data types\n",
    "from pyspark.mllib.linalg import DenseVector, SparseVector, SparseMatrix, DenseMatrix, Vectors, Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great way to get help when using Python is to use the help function on an object or method.  If help isn't sufficient, other places to look include the [programming guides](http://spark.apache.org/docs/latest/programming-guide.html), the [Python API](http://spark.apache.org/docs/latest/api/python/index.html), and directly in the [source code](https://github.com/apache/spark/tree/master/python/pyspark) for PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(Vectors.dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark provides a [DenseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector) class within the module [pyspark.mllib.linalg](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.linalg).  `DenseVector` is used to store arrays of values for use in PySpark.  `DenseVector` actually stores values in a [NumPy array](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html) and delegates calculations to that object.  You can create a new `DenseVector` using `DenseVector()` and passing in an NumPy array or a Python list.\n",
    " \n",
    "`DenseVector` implements several functions, such as `DenseVector.dot()` and `DenseVector.norm()`.\n",
    " \n",
    "Note that `DenseVector` stores all values as `np.float64`, so even if you pass in a NumPy array of integers, the resulting `DenseVector` will contain floating-point numbers. Also, `DenseVector` objects exist locally and are not inherently distributed.  `DenseVector` objects can be used in the distributed setting by including them in `RDDs` or `DataFrames`.\n",
    " \n",
    "You can create a dense vector by using the [Vectors](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors) object and calling `Vectors.dense`.  The `Vectors` object also contains a method for creating `SparseVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a DenseVector using Vectors\n",
    "denseVector = Vectors.dense([1, 2, 3])\n",
    "\n",
    "print 'type(denseVector): {0}'.format(type(denseVector))\n",
    "print '\\ndenseVector: {0}'.format(denseVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dot product **\n",
    " \n",
    "We can calculate the dot product of two vectors, or a vector and itself, by using `DenseVector.dot()`.  Note that the dot product is equivalent to performing element-wise multiplication and then summing the result.\n",
    " \n",
    "Below, you'll find the calculation for the dot product of two vectors, where each vector has length \\\\( n \\\\):\n",
    " \n",
    "\\\\[ w \\cdot x = \\sum_{i=1}^n w_i x_i \\\\]\n",
    " \n",
    "Note that you may also see \\\\( w \\cdot x \\\\) represented as \\\\( w^\\top x \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denseVector.dot(denseVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Norm **\n",
    " \n",
    "We can calculate the norm of a vector using `Vectors.norm`.  The norm calculation is:\n",
    " \n",
    "  \\\\[ ||x|| _p = \\bigg( \\sum_i^n |x_i|^p \\bigg)^{1/p} \\\\]\n",
    " \n",
    " \n",
    " \n",
    "Sometimes we'll want to normalize our features before training a model.  Later on we'll use the `ml` library to perform this normalization using a transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Vectors.norm(denseVector, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, `DenseVector` operations are delegated to an underlying NumPy array so we can perform multiplication, addition, division, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denseVector * denseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "5 + denseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we'll want to treat a vector as an array.  We can convert both sparse and dense vectors to arrays by calling the `toArray` method on the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denseArray = denseVector.toArray()\n",
    "print denseArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'type(denseArray): {0}'.format(type(denseArray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(Vectors.sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `SparseVector` using [Vectors.sparse](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors.sparse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparseVector = Vectors.sparse(10, [2, 7], [1.0, 5.0])\n",
    "print 'type(sparseVector): {0}'.format(type(sparseVector))\n",
    "print '\\nsparseVector: {0}'.format(sparseVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Let's take a look at what fields and methods are available with a `SparseVector`.  Here are links to the [Python](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector) and [Scala](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.mllib.linalg.SparseVector) APIs for `SparseVector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(SparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(SparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(sparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the difference between the two\n",
    "set(dir(sparseVector)) - set(dir(SparseVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inspect is a handy tool for seeing the Python source code\n",
    "import inspect\n",
    "print inspect.getsource(SparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'sparseVector.size: {0}'.format(sparseVector.size)\n",
    "print 'type(sparseVector.size):{0}'.format(type(sparseVector.size))\n",
    "\n",
    "print '\\nsparseVector.indices: {0}'.format(sparseVector.indices)\n",
    "print 'type(sparseVector.indices):{0}'.format(type(sparseVector.indices))\n",
    "print 'type(sparseVector.indices[0]):{0}'.format(type(sparseVector.indices[0]))\n",
    "\n",
    "print '\\nsparseVector.values: {0}'.format(sparseVector.values)\n",
    "print 'type(sparseVector.values):{0}'.format(type(sparseVector.values))\n",
    "print 'type(sparseVector.values[0]):{0}'.format(type(sparseVector.values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't try to set these values directly.  If you use the wrong type, hard-to-debug errors will occur when Spark attempts to use the `SparseVector`. Create a new `SparseVector` using `Vectors.sparse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(dir(DenseVector)) - set(dir(SparseVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denseVector + denseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sparseVector + sparseVector\n",
    "except TypeError as e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparseVector.dot(sparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparseVector.norm(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "In MLlib, labeled training instances are stored using the [LabeledPoint](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) object.  Note that the features and label for a `LabeledPoint` are stored in the `features` and `label` attribute of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "help(LabeledPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeledPoint = LabeledPoint(1992, [3.0, 5.5, 10.0])\n",
    "print 'labeledPoint: {0}'.format(labeledPoint)\n",
    "\n",
    "print '\\nlabeledPoint.features: {0}'.format(labeledPoint.features)\n",
    "# Notice that feaures are being stored as a DenseVector\n",
    "print 'type(labeledPoint.features): {0}'.format(type(labeledPoint.features))\n",
    "\n",
    "print '\\nlabeledPoint.label: {0}'.format(labeledPoint.label)\n",
    "print 'type(labeledPoint.label): {0}'.format(type(labeledPoint.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View the differences between the class and an instantiated instance\n",
    "set(dir(labeledPoint)) - set(dir(LabeledPoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeledPointSparse = LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0}))\n",
    "print 'labeledPointSparse: {0}'.format(labeledPointSparse)\n",
    "\n",
    "print '\\nlabeledPoint.featuresSparse: {0}'.format(labeledPointSparse.features)\n",
    "print 'type(labeledPointSparse.features): {0}'.format(type(labeledPointSparse.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "help(Rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing collaborative filtering we aren't working with vectors or labeled points, so we need another type of object to capture the relationship between users, products, and ratings.  This is represented by a `Rating` which can be found in the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.Rating) and [Scala](https://spark.apache.org/docs/1.5.0/api/scala/index.html#org.apache.spark.mllib.recommendation.Rating) APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print inspect.getsource(Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rating = Rating(4, 10, 2.0)\n",
    "\n",
    "print 'rating: {0}'.format(rating)\n",
    "# Note that we can pull out the fields using a dot notation or indexing\n",
    "print rating.user, rating[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrames\n",
    " \n",
    "When using Spark's ML library rather than MLlib you'll be working with `DataFrames` instead of `RDDs`.  In this section we'll show how you can create a `DataFrame` using MLlib datatypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we saw that `Rating` is a `namedtuple`.  `namedtuples` are useful when working with `DataFrames`.  We'll explore how they work below and use them to create a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "help(namedtuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Address = namedtuple('Address', ['city', 'state'])\n",
    "address = Address('Boulder', 'CO')\n",
    "print 'address: {0}'.format(address)\n",
    "\n",
    "print '\\naddress.city: {0}'.format(address.city)\n",
    "print 'address[0]: {0}'.format(address[0])\n",
    "\n",
    "print '\\naddress.State: {0}'.format(address.state)\n",
    "print 'address[1]: {0}'.format(address[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(sqlContext.createDataFrame([Address('Boulder', 'CO'), Address('New York', 'NY')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `DataFrame` with a couple of rows where the first column is the label and the second is the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LabelAndFeatures = namedtuple('LabelAndFeatures', ['label', 'features'])\n",
    "row1 = LabelAndFeatures(10, Vectors.dense([1.0, 2.0]))\n",
    "row2 = LabelAndFeatures(20, Vectors.dense([1.5, 2.2]))\n",
    "\n",
    "df = sqlContext.createDataFrame([row1, row2])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `DenseVector` with the values 1.5, 2.5, 3.0 (in that order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "denseVec = Vectors.dense([1.5, 2.5, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "from test_helper import Test\n",
    "Test.assertEquals(denseVec, DenseVector([1.5, 2.5, 3.0]), 'incorrect value for denseVec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `LabeledPoint` with a label equal to 10.0 and features equal to `denseVec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "labeledP = LabeledPoint(10.0, denseVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Test.assertEquals(str(labeledP), '(10.0,[1.5,2.5,3.0])', 'incorrect value for labeledP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Challenge Question [Intentionally Hard]**\n",
    " \n",
    "Create a `udf` that pulls the first element out of a column that contains `DenseVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "firstElement = udf(lambda v: float(v[0]), DoubleType())\n",
    "\n",
    "df2 = df.select(firstElement('features').alias('first'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Test.assertEquals(df2.rdd.map(lambda r: r[0]).collect(), [1.0, 1.5], 'incorrect implementation of firstElement')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
