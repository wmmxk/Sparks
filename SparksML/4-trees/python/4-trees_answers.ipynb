{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    " \n",
    "This lab covers decision trees and random forests, while introducing metadata, cross-validation, `StringIndexer`, and `PolynomialExpansion`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data from a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseDir = '/mnt/ml-class/'\n",
    "irisFourFeatures = sqlContext.read.parquet(baseDir + 'irisFourFeatures.parquet')\n",
    "print '\\n'.join(map(repr, irisFourFeatures.take(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data from `SparseVector` to `DenseVector` types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT, DenseVector\n",
    "\n",
    "sparseToDense = udf(lambda sv: Vectors.dense(sv.toArray()), VectorUDT())\n",
    "irisDense = irisFourFeatures.select(sparseToDense('features').alias('features'), 'label')\n",
    "\n",
    "print '\\n'.join(map(repr, irisDense.take(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the new format for use in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#irisDense.write.mode('overwrite').parquet('/tmp/irisDense.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "def prepareSubplot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0, subplots=(1, 1)):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, axList = plt.subplots(subplots[0], subplots[1], figsize=figsize, facecolor='white',\n",
    "                               edgecolor='white')\n",
    "    if not isinstance(axList, np.ndarray):\n",
    "        axList = np.array([axList])\n",
    "\n",
    "    for ax in axList.flatten():\n",
    "        ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "        for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "            axis.set_ticks_position('none')\n",
    "            axis.set_ticks(ticks)\n",
    "            axis.label.set_color('#999999')\n",
    "            if hideLabels: axis.set_ticklabels([])\n",
    "        ax.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "        map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "\n",
    "    if axList.size == 1:\n",
    "        axList = axList[0]  # Just return a single axes object for a regular plot\n",
    "    return fig, axList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = irisDense.collect()\n",
    "features, labels = zip(*data)\n",
    "x1, y1, x2, y2 = zip(*features)\n",
    "\n",
    "colorMap = 'Set1'  # was 'Set2', 'Set1', 'Dark2', 'winter'\n",
    "\n",
    "fig, axList = prepareSubplot(np.arange(-1, 1.1, .2), np.arange(-1, 1.1, .2), figsize=(11., 5.), subplots=(1, 2))\n",
    "ax0, ax1 = axList\n",
    "\n",
    "ax0.scatter(x1, y1, s=14**2, c=labels, edgecolors='#444444', alpha=0.80, cmap=colorMap)\n",
    "ax0.set_xlabel('Sepal Length'), ax0.set_ylabel('Sepal Width')\n",
    "\n",
    "ax1.scatter(x2, y2, s=14**2, c=labels, edgecolors='#444444', alpha=0.80, cmap=colorMap)\n",
    "ax1.set_xlabel('Petal Length'), ax1.set_ylabel('Petal Width')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test sets and visualize the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisTest, irisTrain = irisDense.randomSplit([.30, .70], seed=1)\n",
    "irisTest.cache()\n",
    "irisTrain.cache()\n",
    "\n",
    "print 'Items in test datset: {0}'.format(irisTest.count())\n",
    "print 'Items in train dataset: {0}'.format(irisTrain.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataTrain = irisTrain.collect()\n",
    "featuresTrain, labelsTrain = zip(*dataTrain)\n",
    "x1Train, y1Train, x2Train, y2Train = zip(*featuresTrain)\n",
    "\n",
    "dataTest = irisTest.collect()\n",
    "featuresTest, labelsTest = zip(*dataTest)\n",
    "x1Test, y1Test, x2Test, y2Test = zip(*featuresTest)\n",
    "\n",
    "trainPlot1 = (x1Train, y1Train, labelsTrain, 'Train Data', 'Sepal Length', 'Sepal Width')\n",
    "trainPlot2 = (x2Train, y2Train, labelsTrain, 'Train Data', 'Petal Length', 'Petal Width')\n",
    "testPlot1 = (x1Test, y1Test, labelsTest, 'Test Data', 'Sepal Length', 'Sepal Width')\n",
    "testPlot2 = (x2Test, y2Test, labelsTest, 'Test Data', 'Petal Length', 'Petal Width')\n",
    "plotData = [trainPlot1, testPlot1, trainPlot2, testPlot2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axList = prepareSubplot(np.arange(-1, 1.1, .2), np.arange(-1, 1.1, .2), figsize=(11.,10.), subplots=(2, 2))\n",
    "\n",
    "for ax, pd in zip(axList.flatten(), plotData):\n",
    "    ax.scatter(pd[0], pd[1], s=14**2, c=pd[2], edgecolors='#444444', alpha=0.80, cmap=colorMap)\n",
    "    ax.set_xlabel(pd[4]), ax.set_ylabel(pd[5])\n",
    "    ax.set_title(pd[3], color='#999999')\n",
    "\n",
    "    ax.set_xlim((-1.1, 1.1))\n",
    "    ax.set_ylim((-1.1, 1.1))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the metadata for decision trees and build a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `StringIndexer` on our labels in order to obtain a `DataFrame` that decision trees can work with.  Here are the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StringIndexer) APIs for `StringIndexer`.\n",
    " \n",
    "You'll need to set the input column to \"label\", the output column to \"indexed\", and fit and transform using the `irisTrain` `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = (StringIndexer()\n",
    "                 .setInputCol('label')\n",
    "                 .setOutputCol('indexed'))\n",
    "\n",
    "indexerModel = stringIndexer.fit(irisTrain)\n",
    "irisTrainIndexed = indexerModel.transform(irisTrain)\n",
    "display(irisTrainIndexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "from test_helper import Test\n",
    "Test.assertEquals(irisTrainIndexed.select('indexed').take(50)[-1][0], 2.0, 'incorrect values in indexed column')\n",
    "Test.assertTrue(irisTrainIndexed.schema.fields[2].metadata != {}, 'indexed should have metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've updated the metadata for the field.  Now we know that the field takes on three values and is nominal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print irisTrainIndexed.schema.fields[1].metadata\n",
    "print irisTrainIndexed.schema.fields[2].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a decision tree to classify our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = (DecisionTreeClassifier()\n",
    "      .setLabelCol('indexed')\n",
    "      .setMaxDepth(5)\n",
    "      .setMaxBins(10)\n",
    "      .setImpurity('gini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dt.explainParam('impurity')\n",
    "print '\\n', dt.explainParam('maxBins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View all of the parameters to see if there is anything we'd like to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dt.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model and display predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtModel = dt.fit(irisTrainIndexed)\n",
    "predictionsTest = dtModel.transform(indexerModel.transform(irisTest))\n",
    "display(predictionsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll evaluate the results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "multiEval = (MulticlassClassificationEvaluator()\n",
    "             .setMetricName('precision')\n",
    "             .setLabelCol('indexed'))\n",
    "\n",
    "print multiEval.evaluate(predictionsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtModelString = dtModel._java_obj.toDebugString()\n",
    "print dtModelString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readableModel = dtModelString\n",
    "for feature, name in enumerate(['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']):\n",
    "    readableModel = readableModel.replace('feature {0}'.format(feature), name)\n",
    "\n",
    "print readableModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and find the best cross-validated model.  A `CrossValidator` requires an estimator to build the models, an evaluator to compare the performance of the models, a parameter grid that specifies which estimator parameters to tune, and the number of folds to use.\n",
    " \n",
    "There is a good example in the [ML Guide](http://spark.apache.org/docs/latest/ml-guide.html#example-model-selection-via-cross-validation), although it is only in Scala.  The Python code is very similar.\n",
    " \n",
    "The estimator that we will use is a `Pipeline` that has `stringIndexer` and `dt`.\n",
    " \n",
    "The evaluator will be `multiEval`.  You just need to make sure the metric is \"precision\".\n",
    " \n",
    "We'll use `ParamGridBuilder` to build a grid with `dt.maxDepth` values of 2, 4, 6, and 10 (in that order).\n",
    " \n",
    "Finally, we'll use 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "cvPipeline = Pipeline().setStages([stringIndexer, dt])\n",
    "\n",
    "multiEval.setMetricName('precision')\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [2, 4, 6, 10])\n",
    "             .build())\n",
    "\n",
    "cv = (CrossValidator()\n",
    "      .setEstimator(cvPipeline)\n",
    "      .setEvaluator(multiEval)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setNumFolds(5))\n",
    "\n",
    "cvModel = cv.fit(irisTrain)\n",
    "predictions = cvModel.transform(irisTest)\n",
    "print multiEval.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Test.assertEquals(round(multiEval.evaluate(predictions), 2), .98, 'incorrect predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was our best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestDTModel = cvModel.bestModel.stages[-1]\n",
    "print bestDTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see more details on what parameters were used to build the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print bestDTModel._java_obj.parent().explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print bestDTModel._java_obj.parent().getMaxDepth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest and `PolynomialExpansion`\n",
    " \n",
    "Next, we'll build a random forest.  Since we only have a few features and random forests tend to work better with a lot of features, we'll expand our features using `PolynomialExpansion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "px = (PolynomialExpansion()\n",
    "      .setInputCol('features')\n",
    "      .setOutputCol('polyFeatures'))\n",
    "\n",
    "print px.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the `RandomForestClassifier` to build our random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = (RandomForestClassifier()\n",
    "      .setLabelCol('indexed')\n",
    "      .setFeaturesCol('polyFeatures'))\n",
    "\n",
    "print rf.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set some params based on what we just read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(rf\n",
    " .setMaxBins(10)\n",
    " .setMaxDepth(2)\n",
    " .setNumTrees(20)\n",
    " .setSeed(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll build a pipeline that includes the `StringIndexer`, `PolynomialExpansion`, and `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfPipeline = Pipeline().setStages([stringIndexer, px, rf])\n",
    "rfModelPipeline = rfPipeline.fit(irisTrain)\n",
    "rfPredictions = rfModelPipeline.transform(irisTest)\n",
    "\n",
    "print multiEval.evaluate(rfPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(rfPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what exactly did `PolynomialExpansion` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All interactions\n",
    " \n",
    "\\\\[ \\begin{bmatrix} a \\times a & b \\times a & c \\times a & d \\times a \\\\\\ a \\times b & b \\times b & c \\times b & d \\times b \\\\\\ a \\times c & b \\times c & c \\times c & d \\times c \\\\\\ a \\times d & b \\times d & c \\times d & d \\times d \\end{bmatrix}  \\\\]\n",
    " \n",
    "Remove duplicates\n",
    " \n",
    "\\\\[ \\begin{bmatrix} a \\times a \\\\\\ a \\times b & b \\times b \\\\\\ a \\times c & b \\times c & c \\times c \\\\\\ a \\times d & b \\times d & c \\times d & d \\times d \\end{bmatrix}  \\\\]\n",
    " \n",
    "Plus the original features\n",
    " \n",
    "\\\\[ \\begin{bmatrix} a & b & c & d \\end{bmatrix} \\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better?  Let's build a grid of params and search using `CrossValidator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramGridRand = (ParamGridBuilder()\n",
    "                 .addGrid(rf.maxDepth, [2, 4, 8, 12])\n",
    "                 .baseOn({rf.numTrees, 20})\n",
    "                 .build())\n",
    "\n",
    "cvRand = (CrossValidator()\n",
    "          .setEstimator(rfPipeline)\n",
    "          .setEvaluator(multiEval)\n",
    "          .setEstimatorParamMaps(paramGridRand)\n",
    "          .setNumFolds(2))\n",
    "\n",
    "cvModelRand = cvRand.fit(irisTrain)\n",
    "predictionsRand = cvModelRand.transform(irisTest)\n",
    "print multiEval.evaluate(predictionsRand)\n",
    "print cvModelRand.bestModel.stages[-1]._java_obj.parent().getMaxDepth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's view the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cvModelRand.bestModel.stages[-1]._java_obj.toDebugString()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
